{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TorchData Demo",
      "provenance": [],
      "collapsed_sections": [
        "vwC-VypRDYZM"
      ],
      "authorship_tag": "ABX9TyPvO1YPwxa6Seb0kRyqIkVY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dracifer/td_exploration/blob/main/TorchData_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Option 1 -- Cohesive with current Dataloader V2 design:\n",
        "* Business logic expressed by `Datapipe` (or conventional `Dataset`)\n",
        "  * Can be traced to a backend independent execution plan/graph -- *Logical IR*\n",
        "  * With current design that Datapipe is modeled as a iterable, we essentially do not have eager mode\n",
        "\n",
        "```\n",
        "class Datapipe:\n",
        "    def shuffle(self, shuffle_spec: ShuffleSpec) -> \"Datapipe\":\n",
        "    def repeat(self, num: int) -> \"Datapipe\":\n",
        "    def sort(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    def map_batches(self, fn: Callable[[RowBatch], RowBatch], batch_size: Optional[int] = None) -> \"Datapipe\":\n",
        "    def filter(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    # aggregation functions like max, mean, min, sum, etc. will be added later\n",
        "```\n",
        "\n",
        "\n",
        "* Execution specified by `ReadingService`\n",
        "  * Can adapt *Logical IR* to a backend *Execution Plan* \n",
        "\n",
        "```\n",
        "class ReadingService(ABC):\n",
        "  def initialize(self, dp: Datapipe) -> Datapipe:\n",
        "  def finalize(self) -> bool:\n",
        "  def iter_batches(self, dp: Datapipe, state: Dict[str, Any]) -> RowBatchIterator:\n",
        "```\n",
        "```   \n",
        "class SimpleReadingService(ReadingService):\n",
        "class MultiprocessingReadingService(ReadingService):\n",
        "class OnboxDppReadingService(ReadingService):\n",
        "class DisaggDppReadingService(ReadingService):\n",
        "class RayDppReadingService(ReadingService):\n",
        "```\n",
        "* Connect business logic (specified by `Datapipe`) with execution engine (specified by `ReadingService`) and produce data through iteration by `Dataloader`\n",
        "\n",
        "```\n",
        "class Dataloader:\n",
        "  def __init__(self, datapipe: Datapipe, reading_service: ReadingService) -> None:\n",
        "  def reset(self) -> None:\n",
        "  def iter_batches(self) -> RowBatchIterator:\n",
        "  def __iter__(self) -> \"Dataloader\":\n",
        "  def __next__(self) -> RowBatch:\n",
        "  def state_dict(self) -> Dict[str, Any]:\n",
        "  def load_state_dict(self, state: Dict[str, Any]) -> bool:\n",
        "```\n",
        "\n",
        "### Note\n",
        "* Ray Dataset is a combination of all the 3 concepts above\n",
        "* Ray DatasetPipeline is a parallelism mechanism that pipelining operations on batches. This is to overlapping data reading with training. We will ignore this use case in this doc. There is similar implementation in TorchRec. Will have follow-up design on enabling high efficient pipelining later."
      ],
      "metadata": {
        "id": "jV-9LAZQkT5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install libs"
      ],
      "metadata": {
        "id": "GV-ab990sksg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw0zZ0LVthvj",
        "outputId": "402702d8-69d4-47d9-8990-afea15be9e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "2YprvdjGrC_e",
        "outputId": "9d18673c-0a8a-4ef3-d49e-3a0fa51d886d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow==7.0.0\n",
            "  Downloading pyarrow-7.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.7 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 6.0.1\n",
            "    Uninstalling pyarrow-6.0.1:\n",
            "      Successfully uninstalled pyarrow-6.0.1\n",
            "Successfully installed pyarrow-7.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip3 install pyarrow==7.0.0 pandas numpy \n",
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "2B8yDR0Dspwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from enum import Enum\n",
        "from typing import Any, Collection, Dict, Iterable, Iterator, List, Optional, Tuple, Type, Union, cast, Mapping, Callable\n",
        "from collections import UserDict\n",
        "\n",
        "def compute_offsets(input: Iterable, include_last: bool) -> List[int]:\n",
        "    offsets = list(itertools.accumulate([0] + input, lambda x, y: x + len(y)))\n",
        "    if not include_last:\n",
        "        return offsets[:-1]\n",
        "    return offsets\n",
        "\n",
        "\n",
        "def is_primitive(input: Any) -> bool:\n",
        "    PRIMITIVE = (int, str, bool, float)\n",
        "    if isinstance(input, PRIMITIVE):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "class StructType(Enum):\n",
        "    NON_LEAF = 0\n",
        "    LEAF_FLOAT = 1\n",
        "    LEAF_ID_LIST = 2\n",
        "    LEAF_ID_SCORE_MAP = 3\n",
        "\n",
        "\n",
        "def struct_type(input: Any) -> bool:\n",
        "    if type(input) == float:\n",
        "        return StructType.LEAF_FLOAT\n",
        "    if isinstance(input, dict) and (\n",
        "        len(input) == 0 or (type(next(iter(input.keys()))), type(next(iter(input.values())))) == (int, float)\n",
        "    ):\n",
        "        return StructType.LEAF_ID_SCORE_MAP\n",
        "    if type(input) == list and (len(input) == 0 or type(next(iter(input))) == int):\n",
        "        return StructType.LEAF_ID_LIST\n",
        "    return StructType.NON_LEAF\n",
        "    "
      ],
      "metadata": {
        "id": "qCCmKIZqrXQF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datapipe: Data logical processor\n",
        "* Datapipe describes operations. It is STATELESS.\n",
        "The operation plan/IR can be compiled and reused (not implemented yet). Both training and inference logic can be described by Datapipe but should have different data source and operations (e.g. training specifies shuffle, while inference does not)\n",
        "\n",
        "* RowBatch: A batch of rows. "
      ],
      "metadata": {
        "id": "UaFK6TqKssws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow\n",
        "import torch\n",
        "from typing import TypeVar\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "def to_torch(input: Union[dict, pyarrow.Array]) -> Union[dict, tuple]:\n",
        "  if type(input) == pyarrow.lib.FloatArray:\n",
        "    return (\n",
        "        torch.ones(len(input), dtype=torch.int8),\n",
        "        torch.from_numpy(input.to_numpy()),\n",
        "    )\n",
        "  elif type(input) == pyarrow.lib.ListArray:\n",
        "    return (\n",
        "        torch.from_numpy(input.offsets.to_numpy()),\n",
        "        torch.from_numpy(input.values.to_numpy()),\n",
        "    )\n",
        "  elif type(input) == pyarrow.lib.MapArray:\n",
        "    return (\n",
        "        torch.from_numpy(input.offsets.to_numpy()),\n",
        "        torch.from_numpy(input.keys.to_numpy()),\n",
        "        torch.from_numpy(input.items.to_numpy()),\n",
        "    )\n",
        "\n",
        "  output = {}\n",
        "  if isinstance(input, dict):\n",
        "    for key, val in input.items():\n",
        "      output[key] = to_torch(val)\n",
        "  return output\n",
        "\n",
        "\n",
        "Row = dict\n",
        "\n",
        "# collate operation such as to_torch() can be done by batch data OR\n",
        "# By building the operation as a part of execution plan by Datapipe so that it\n",
        "# will be conducted on server side.\n",
        "# Here we use RowBatch.to_torch() for demonstration purpose\n",
        "class RowBatch(dict):\n",
        "  def __init__(self, data, row_num: int):\n",
        "    self._row_num = row_num\n",
        "    super().__init__(data)\n",
        "    return\n",
        "\n",
        "  @property\n",
        "  def row_num(self):\n",
        "    return self._row_num\n",
        "\n",
        "  def to_torch(self, schema: Optional[dict] = None) -> dict:\n",
        "    if schema is not None:\n",
        "      raise NotImplementedError(\"not supporting customized schema\")\n",
        "    return to_torch(self)\n",
        "        \n",
        "\n",
        "class RowBatchIterator(Iterator[RowBatch]):\n",
        "  def __init__(self, fetch_fn: Callable[[Dict[str, Any]], RowBatch], states: Dict[str, Any]) -> None:\n",
        "    self._fetch_fn = fetch_fn\n",
        "    self._orig_states = states\n",
        "    self._states = states\n",
        "\n",
        "  def __iter__(self) -> \"RowBatchIterator\": \n",
        "    # reset iterator\n",
        "    self._states = self._orig_states\n",
        "    return self\n",
        "\n",
        "  def __next__(self) -> RowBatch:\n",
        "    data, self._states = self._fetch_fn(self._states)\n",
        "    return data\n",
        "\n",
        "\n",
        "class ShuffleSpec(Enum):\n",
        "  NO_SHUFFLE = 0\n",
        "  FULL_SHUFFLE = 1\n",
        "\n",
        "class Datapipe(ABC):\n",
        "  # Putting compute_adaptor in ctor for now. In this way, each plan modification can be applied directly\n",
        "  # to fit the specific adaptor need.\n",
        "  # An alternative is to build compute engine (e.g. Apache Arrow, TorchArrow, etc) agonostic \n",
        "  # plan, and specify compute_adaptor in map_batches() similar to what Ray does. \n",
        "  def __init__(self, compute_adaptor) -> None:\n",
        "    self._compute_adaptor = compute_adaptor\n",
        "    self._batch_size = 3\n",
        "\n",
        "  # Commenting out all plan building logic for now.\n",
        "  def shuffle(self, shuffle_spec: ShuffleSpec) -> \"Datapipe\":\n",
        "    self._shuffle_spec = shuffle_spec\n",
        "    # self._plan = PlanBuilder.addShuffle(shuffle_spec)\n",
        "    return self\n",
        "\n",
        "  def repeat(self, num: int) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addRepeat(num)\n",
        "    self._total_passes = num\n",
        "    return self\n",
        "\n",
        "  def sort(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addSort(fn)\n",
        "    return self\n",
        "\n",
        "  def map_batches(self, fn: Callable[[RowBatch], RowBatch], batch_size: Optional[int] = None) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addMapBatches(fn, batch_size)\n",
        "    self._batch_size = batch_size or self._batch_size\n",
        "    self._map_batch_fn = fn\n",
        "    return self\n",
        "\n",
        "  # can be implemented with map_batches()\n",
        "  def filter(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addFilter(fn)\n",
        "    return self\n",
        "\n",
        "  # can be implemented with map_batches()\n",
        "  def map(self, fn: Callable[[Row], Row]) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addMap(fn)\n",
        "    return self\n",
        "\n",
        "  # aggregation functions like max, mean, min, sum, etc. will be added later\n",
        "\n",
        "  @property\n",
        "  def batch_size(self) -> int:\n",
        "    return self._batch_size\n",
        "\n",
        "# IterDatapipe -- iterate over data source to produce data. \n",
        "# Used in training together with Dataloader and ReadingService\n",
        "class IterDatapipe(Datapipe, ABC):\n",
        "  @abstractmethod\n",
        "  def iter_batches(self, batch_size: int, start: int = 0, stride: int = 1) -> dict:\n",
        "    pass\n",
        "\n",
        "\n",
        "class PyDictDatapipe(IterDatapipe): \n",
        "  def __init__(self, data: Dict[str, Any], block_size: int, compute_adaptor) -> None:\n",
        "    self._data = data\n",
        "    self._block_size = block_size\n",
        "    super().__init__(compute_adaptor)\n",
        "\n",
        "  def fetch_batch(self, states: Dict[str, Any]) -> Tuple[RowBatch, Dict[str, Any]]:\n",
        "    # The current implementation is for demo purpose. \n",
        "    # Real implementation should be based on executing self._plan rather than calling specific functions\n",
        "    batch = self._compute_adaptor.build_columns(self._data, states[\"batch_size\"], states[\"start\"])\n",
        "    if hasattr(self, \"_map_batch_fn\"):\n",
        "      batch = self._map_batch_fn(batch)\n",
        "    states[\"start\"] += states[\"batch_size\"] * states[\"stride\"]\n",
        "    return (batch, states)\n",
        "\n",
        "  def iter_batches(self, batch_size: int, start: int = 0, stride: int = 1) -> RowBatchIterator:\n",
        "    return RowBatchIterator(self.fetch_batch, {\"start\": start, \"batch_size\": batch_size, \"stride\": stride})\n",
        "\n",
        "# class HiveDatapipe(IterDatapipe): \n",
        "\n",
        "# CallableDatapipe -- Apply transformation on data. \n",
        "# Action in reactive way (need to be called with provided data). Userd in inference\n",
        "class CallableDatapipe(Datapipe, ABC):\n",
        "  def __call__(self, data):\n",
        "    raise NotImplementedError(\"not supporting customized schema\")\n",
        "\n",
        "class SchemaDatapipe(CallableDatapipe):\n",
        "  def __init__(self, schema: Dict[str, Any], compute_adaptor) -> None:\n",
        "    self._schema = schema\n",
        "    super().__init__(compute_adaptor)\n",
        "\n",
        "  def conversion(self, data) -> RowBatch:\n",
        "    # The current implementation is for demo purpose. \n",
        "    # Real implementation should be based on executing self._plan rather than calling specific functions\n",
        "    batch = self._compute_adaptor.build_columns(data, None, 0)\n",
        "    if hasattr(self, \"_map_batch_fn\"):\n",
        "      batch = self._map_batch_fn(batch)\n",
        "    return batch\n",
        "\n",
        "  def __call__(self, data) -> RowBatch: \n",
        "    return self.conversion(data)"
      ],
      "metadata": {
        "id": "VuQFXjQWrKKG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datapipe Adaptor\n",
        "\n",
        "Adapt Datapipe with particular expression evaluation engine. Use Apache Arrow for demo\n"
      ],
      "metadata": {
        "id": "L8bfI-08I2Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt Datapipes with expression evaluation engine. \n",
        "# It adapts IterDatapipe and CallableDatapipe differently.\n",
        "# In this example, PyDictArrowDatapipeAdaptor constructs Arrow data from Python dict. \n",
        "class PyDictArrowDatapipeAdaptor:\n",
        "  @staticmethod\n",
        "  def build_column_val(input: List[Any]) -> pyarrow.Array:\n",
        "      child_type = struct_type(next(iter(input)))\n",
        "      assert child_type != StructType.NON_LEAF, f\"Invalid input {input}\"\n",
        "      # List[List[int]]\n",
        "      if child_type == StructType.LEAF_ID_LIST:\n",
        "          col = pyarrow.ListArray.from_arrays(\n",
        "              offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "              values=pyarrow.array(list(itertools.chain(*input)), type=pyarrow.int64()),\n",
        "          )\n",
        "      # List[float]\n",
        "      elif child_type == StructType.LEAF_FLOAT:\n",
        "          col = pyarrow.array(input, type=pyarrow.float32())\n",
        "      # List[Dict[int, float]]\n",
        "      elif child_type == StructType.LEAF_ID_SCORE_MAP:\n",
        "          col = pyarrow.MapArray.from_arrays(\n",
        "              offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "              keys=pyarrow.array(list(itertools.chain(*[item.keys() for item in input])), type=pyarrow.int64()),\n",
        "              items=pyarrow.array(list(itertools.chain(*[item.values() for item in input])), type=pyarrow.float32()),\n",
        "          )\n",
        "      return col\n",
        "\n",
        "  @staticmethod\n",
        "  def build_columns(input: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "      assert len(input) > 0, f\"input {input} cannot have empty columns\"\n",
        "      col_values = []\n",
        "      row_num = block_size\n",
        "      for val in input.values():\n",
        "          if type(val) == list:\n",
        "              block_size = block_size or len(val)\n",
        "              end = min(start + block_size, len(val))\n",
        "              row_num = end - start\n",
        "              col_val = PyDictArrowDatapipeAdaptor.build_column_val(val[start:end])\n",
        "          elif type(val) == dict:\n",
        "              col_val = PyDictArrowDatapipeAdaptor.build_columns(val, block_size, start)\n",
        "              row_num = col_val.row_num\n",
        "          col_values.append(col_val)\n",
        "\n",
        "      return RowBatch(data=zip(input.keys(), col_values), row_num=row_num)"
      ],
      "metadata": {
        "id": "VIwifsz6I2uf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Data"
      ],
      "metadata": {
        "id": "JpvPivsgZuwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SCHEMA = (\n",
        "    (\"float_features\", Dict[str, List[float]]),\n",
        "    (\"float_features\", Dict[str, List[List[int]]]),\n",
        "    (\"id_score_list_features\", Dict[str, List[Dict[int, float]]]),\n",
        ")\n",
        "\n",
        "\n",
        "# dp = torch.data.from_hive(table_ns=\"ai_platform\", table_nm=\"table_1\", partitions=[\"ds='2022-04-23'\"])\n",
        "# 2 columns, 6 rows with 3 rows being a block\n",
        "dp = PyDictDatapipe(\n",
        "    data={\n",
        "        \"float_features\": {\n",
        "            \"f1\": [1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
        "            \"f2\": [2.0, 2.1, 2.2, 2.3, 2.4, 2.5],\n",
        "        },\n",
        "        \"id_list_features\": {\n",
        "            \"id1\": [[111, 112], [121], [131, 132], [], [151], [161]],\n",
        "            \"id2\": [[], [221], [222], [223, 224], [225], []],\n",
        "            \"id3\": [[311], [], [331], [341], [351], [361]],\n",
        "        },\n",
        "        \"id_score_list_features\": {\n",
        "            \"ids1\": [{411: 0.1}, {421: 0.2}, {431: 0.7}, {}, {451: 0.4, 452: 0.1}, {}],\n",
        "        },\n",
        "    },\n",
        "    block_size=4,\n",
        "    compute_adaptor=PyDictArrowDatapipeAdaptor(),\n",
        ")"
      ],
      "metadata": {
        "id": "APQLSL-gs3ik"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test single iter\n",
        "rb = next(iter(dp.iter_batches(batch_size=4, start=0, stride=1)))\n",
        "type(rb)\n",
        "rb.row_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihOdG6wptOPC",
        "outputId": "788587af-6d23-430b-a4fd-2602a6593b4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.compute as F\n",
        "\n",
        "print(rb[\"float_features\"][\"f1\"])\n",
        "print(F.ln(F.add(rb[\"float_features\"][\"f1\"], 1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etchmkzns8bR",
        "outputId": "47624d9e-cb34-4c8f-ba06-c0bbd4dc7e62"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  1,\n",
            "  1.1,\n",
            "  1.2,\n",
            "  1.3\n",
            "]\n",
            "[\n",
            "  0.6931472,\n",
            "  0.7419373,\n",
            "  0.7884574,\n",
            "  0.8329091\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test iter_batches\n",
        "for batch in dp.iter_batches(batch_size=5):\n",
        "  print(batch[\"float_features\"][\"f1\"])\n",
        "  print(batch.row_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PQykZIjzChC",
        "outputId": "a6177e56-bcba-429e-a6b8-35ce80f8d026"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  1,\n",
            "  1.1,\n",
            "  1.2,\n",
            "  1.3,\n",
            "  1.4\n",
            "]\n",
            "5\n",
            "[\n",
            "  1.5\n",
            "]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReadingService\n",
        "Specify the data reading execution engine. It should be STATELESS\n",
        "\n",
        "\n",
        "* SimpleReadingService\n",
        "* DisaggDppReadingService\n",
        "* OnboxDppReadingService\n",
        "* OnboxMultiprocessReadingService"
      ],
      "metadata": {
        "id": "w1n7y89owsf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReadingService(ABC):\n",
        "  def __init__(self) -> None:\n",
        "    \n",
        "    return\n",
        "\n",
        "  def initialize(self, dp: IterDatapipe) -> IterDatapipe:\n",
        "    raise NotImplementedError(\"Not implement initialize()\")\n",
        "\n",
        "  def finalize(self) -> bool:\n",
        "    return True\n",
        "\n",
        "  def iter_batches(self, dp: IterDatapipe, state: Dict[str, Any]) -> RowBatchIterator:\n",
        "    raise NotImplementedError(\"Not implement read_batch()\")\n",
        "\n",
        "\n",
        "class SimpleReadingService(ReadingService):\n",
        "  def initialize(self, dp: Datapipe) -> IterDatapipe:\n",
        "    return dp\n",
        "\n",
        "  def iter_batches(self, dp: IterDatapipe, state: Dict[str, Any]) -> RowBatchIterator:\n",
        "    yield from dp.iter_batches(batch_size=dp.batch_size, start=state[\"start\"], stride=1)\n"
      ],
      "metadata": {
        "id": "zWEy4RyCwqWW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader\n",
        "* Take Datapipe (describing business logic) and ReadingService (specifying execution engine) to produce data batches. \n",
        "* Dataloader is STATEFUL\n",
        "* Provide checkpointing interface"
      ],
      "metadata": {
        "id": "EMjCqevIx_GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataloader:\n",
        "  def __init__(self, datapipe: Datapipe, reading_service: ReadingService) -> None:\n",
        "    self._dp = datapipe\n",
        "    self._rs = reading_service\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self) -> None:\n",
        "    self._states = {\n",
        "        \"start\": 0,\n",
        "    }\n",
        "    self._dp = self._rs.initialize(self._dp)\n",
        "\n",
        "  def iter_batches(self) -> RowBatchIterator:\n",
        "    # TODO: need to update self._states appropriately\n",
        "    yield from self._rs.iter_batches(self._dp, self._states)\n",
        "\n",
        "  def __iter__(self) -> \"Dataloader\":\n",
        "    self.reset()\n",
        "    return self\n",
        "\n",
        "  def __next__(self) -> RowBatch:\n",
        "    return next(self.iter_batches())\n",
        "\n",
        "  def state_dict(self) -> Dict[str, Any]:\n",
        "    return self._states\n",
        "\n",
        "  def load_state_dict(self, states: Dict[str, Any]) -> bool:\n",
        "    self._states = states\n",
        "    return True"
      ],
      "metadata": {
        "id": "GmJeviKSyQqK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "jhzqtpmrbEc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.compute as F\n",
        "\n",
        "def collate(rows: RowBatch) -> RowBatch:\n",
        "  return rows.to_torch()\n",
        "\n",
        "def preproc(rows: RowBatch) -> RowBatch:\n",
        "    rows[\"float_features\"][\"f1\"] = F.ln(F.add(rows[\"float_features\"][\"f1\"], 3))\n",
        "    # can collate in preproc. Or can collate in training loop.\n",
        "    output = collate(rows)\n",
        "    return output\n",
        "\n",
        "data={\n",
        "    \"float_features\": {\n",
        "        \"f1\": [1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
        "        \"f2\": [2.0, 2.1, 2.2, 2.3, 2.4, 2.5],\n",
        "    },\n",
        "    \"id_list_features\": {\n",
        "        \"id1\": [[111, 112], [121], [131, 132], [], [151], [161]],\n",
        "        \"id2\": [[], [221], [222], [223, 224], [225], []],\n",
        "        \"id3\": [[311], [], [331], [341], [351], [361]],\n",
        "    },\n",
        "    \"id_score_list_features\": {\n",
        "        \"ids1\": [{411: 0.1}, {421: 0.2}, {431: 0.7}, {}, {451: 0.4, 452: 0.1}, {}],\n",
        "    },\n",
        "}\n",
        "dp = PyDictDatapipe(\n",
        "    data=data,\n",
        "    block_size=4,\n",
        "    compute_adaptor=PyDictArrowDatapipeAdaptor(),\n",
        ")\n",
        "# dp = dp.repeat(num=2)\n",
        "# dp = dp.shuffle(shuffle_mode=ShuffleMode.SHAFFLE_ALL)\n",
        "dp = dp.map_batches(preproc, batch_size=5)\n",
        "\n",
        "rs = SimpleReadingService()\n",
        "\n",
        "dl = Dataloader(dp, rs)\n",
        "\n",
        "for idx, batch in enumerate(dl.iter_batches()):\n",
        "  # print(f\"==Batch {idx}, size {batch.row_num}==\")\n",
        "  # input = batch.to_torch()\n",
        "  print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW43mh7GJCsc",
        "outputId": "9c61d0f3-f000-4508-c696-af90e1d2cca5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'float_features': {'f1': (tensor([1, 1, 1, 1, 1], dtype=torch.int8), tensor([1.3863, 1.4110, 1.4351, 1.4586, 1.4816])), 'f2': (tensor([1, 1, 1, 1, 1], dtype=torch.int8), tensor([2.0000, 2.1000, 2.2000, 2.3000, 2.4000]))}, 'id_list_features': {'id1': (tensor([0, 2, 3, 5, 5, 6], dtype=torch.int32), tensor([111, 112, 121, 131, 132, 151])), 'id2': (tensor([0, 0, 1, 2, 4, 5], dtype=torch.int32), tensor([221, 222, 223, 224, 225])), 'id3': (tensor([0, 1, 1, 2, 3, 4], dtype=torch.int32), tensor([311, 331, 341, 351]))}, 'id_score_list_features': {'ids1': (tensor([0, 1, 2, 3, 3, 5], dtype=torch.int32), tensor([411, 421, 431, 451, 452]), tensor([0.1000, 0.2000, 0.7000, 0.4000, 0.1000]))}}\n",
            "{'float_features': {'f1': (tensor([1], dtype=torch.int8), tensor([1.5041])), 'f2': (tensor([1], dtype=torch.int8), tensor([2.5000]))}, 'id_list_features': {'id1': (tensor([0, 1], dtype=torch.int32), tensor([161])), 'id2': (tensor([0, 0], dtype=torch.int32), tensor([], dtype=torch.int64)), 'id3': (tensor([0, 1], dtype=torch.int32), tensor([361]))}, 'id_score_list_features': {'ids1': (tensor([0, 0], dtype=torch.int32), tensor([], dtype=torch.int64), tensor([]))}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MaM0SUoVjrws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infernece"
      ],
      "metadata": {
        "id": "nr66zYTmibkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dp = SchemaDatapipe(schema=INPUT_SCHEMA, compute_adaptor=InmemoryArrowDatapipeAdaptor())\n",
        "dp = dp.map_batches(preproc)\n",
        "pred = dp(data)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfUnQpBsidOO",
        "outputId": "dcc3e88d-6544-4222-ce79-64f32494c006"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'float_features': {'f1': (tensor([1, 1, 1, 1, 1, 1], dtype=torch.int8),\n",
              "   tensor([1.3863, 1.4110, 1.4351, 1.4586, 1.4816, 1.5041])),\n",
              "  'f2': (tensor([1, 1, 1, 1, 1, 1], dtype=torch.int8),\n",
              "   tensor([2.0000, 2.1000, 2.2000, 2.3000, 2.4000, 2.5000]))},\n",
              " 'id_list_features': {'id1': (tensor([0, 2, 3, 5, 5, 6, 7], dtype=torch.int32),\n",
              "   tensor([111, 112, 121, 131, 132, 151, 161])),\n",
              "  'id2': (tensor([0, 0, 1, 2, 4, 5, 5], dtype=torch.int32),\n",
              "   tensor([221, 222, 223, 224, 225])),\n",
              "  'id3': (tensor([0, 1, 1, 2, 3, 4, 5], dtype=torch.int32),\n",
              "   tensor([311, 331, 341, 351, 361]))},\n",
              " 'id_score_list_features': {'ids1': (tensor([0, 1, 2, 3, 3, 5, 5], dtype=torch.int32),\n",
              "   tensor([411, 421, 431, 451, 452]),\n",
              "   tensor([0.1000, 0.2000, 0.7000, 0.4000, 0.1000], dtype=torch.float64))}}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archived"
      ],
      "metadata": {
        "id": "vwC-VypRDYZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class Table(ABC, dict):\n",
        "    def __init__(self, data: Dict[str, Any], block_size: int) -> None: # , start: int = 0, stride: int = 1) -> None:\n",
        "        self._data = data\n",
        "        self._block_size = block_size\n",
        "        # self._start = start\n",
        "        # self._stride = stride\n",
        "\n",
        "    # def __iter__(self) -> RowBatchIterator:\n",
        "    #     return RowBatchIterator(self, self._start, self._block_size, self._stride)\n",
        "\n",
        "    # TODO: wonder if we should have iterator method in Table?\n",
        "    #       considering Table is an \"accessors of data\", but not necessary storing the real data, having it seems reasonable\n",
        "    def iter_batches(self, batch_size: int, start: int = 0, stride: int = 1) -> RowBatchIterator:\n",
        "      return RowBatchIterator(self, start=start, batch_size=batch_size, stride=stride)\n",
        "\n",
        "    # TODO: this fucntion will be called by RowBatchIterator.__next__(). Seems a bit cyclic...\n",
        "    @abstractmethod\n",
        "    def fetch_batch(self, start: int, batch_size: int) -> RowBatch:\n",
        "      ...\n",
        "\n",
        "\n",
        "class ArrowTable(Table):\n",
        "    # def __next__(self) -> RowBatch:\n",
        "    #     columns = ArrowTable.build_columns(self._data, self._block_size, self._start)\n",
        "    #     self._start += self._block_size * self._stride\n",
        "    #     return columns\n",
        "    \n",
        "    def fetch_batch(self, start: int, batch_size: int) -> RowBatch:\n",
        "      return ArrowTable.build_columns(self._data, batch_size, start)\n",
        "\n",
        "    @staticmethod\n",
        "    def build_column_val(input: List[Any]) -> pyarrow.Array:\n",
        "        child_type = struct_type(next(iter(input)))\n",
        "        assert child_type != StructType.NON_LEAF, f\"Invalid input {input}\"\n",
        "        # List[List[int]]\n",
        "        if child_type == StructType.LEAF_ID_LIST:\n",
        "            col = pyarrow.ListArray.from_arrays(\n",
        "                offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "                values=list(itertools.chain(*input)),\n",
        "            )\n",
        "        # List[float]\n",
        "        elif child_type == StructType.LEAF_FLOAT:\n",
        "            col = pyarrow.array(input, type=pyarrow.float32())\n",
        "        # List[Dict[int, float]]\n",
        "        elif child_type == StructType.LEAF_ID_SCORE_MAP:\n",
        "            col = pyarrow.MapArray.from_arrays(\n",
        "                offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "                keys=list(itertools.chain(*[item.keys() for item in input])),\n",
        "                items=list(itertools.chain(*[item.values() for item in input])),\n",
        "            )\n",
        "        return col\n",
        "\n",
        "    @staticmethod\n",
        "    def build_columns(input: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "        assert len(input) > 0, f\"input {input} cannot have empty columns\"\n",
        "        col_values = []\n",
        "        row_num = block_size\n",
        "        for val in input.values():\n",
        "            if type(val) == list:\n",
        "                block_size = block_size or len(val)\n",
        "                end = min(start + block_size, len(val))\n",
        "                row_num = end - start\n",
        "                col_val = ArrowTable.build_column_val(val[start:end])\n",
        "            elif type(val) == dict:\n",
        "                col_val = ArrowTable.build_columns(val, block_size, start)\n",
        "                row_num = col_val.row_num\n",
        "            col_values.append(col_val)\n",
        "\n",
        "        return RowBatch(data=zip(input.keys(), col_values), row_num=row_num)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_pydict(input: Dict[str, Any], block_size: int) -> \"ArrowTable\":\n",
        "        return ArrowTable(data=input, block_size=block_size)"
      ],
      "metadata": {
        "id": "PcvYtV_CDXf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}