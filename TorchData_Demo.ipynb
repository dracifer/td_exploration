{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TorchData Demo",
      "provenance": [],
      "collapsed_sections": [
        "vwC-VypRDYZM"
      ],
      "authorship_tag": "ABX9TyNqgj6+F3jrl1VHh6JIAJi8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dracifer/td_exploration/blob/main/TorchData_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Option 1 -- Cohesive with current Dataloader V2 design:\n",
        "* Business logic expressed by `Datapipe` (or conventional `Dataset`)\n",
        "  * Can be traced to a backend independent execution plan/graph -- *Logical IR*\n",
        "  * Function calls like `shuffle` and `map_batches` accumulately build the plan rother than execute immediately. Actual data loading and processing is triggered by an \"execution\" call. \n",
        "  * Datapipe can be used to compose plans for both training and inference. In certain cases (e.g. eager serving mode), Datapipe is not needed. \n",
        "```\n",
        "class Datapipe:\n",
        "    def shuffle(self, shuffle_spec: ShuffleSpec) -> \"Datapipe\":\n",
        "    def repeat(self, num: int) -> \"Datapipe\":\n",
        "    def sort(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    def map_batches(self, fn: Callable[[RowBatch], RowBatch], batch_size: Optional[int] = None) -> \"Datapipe\":\n",
        "    def filter(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    # aggregation functions like max, mean, min, sum, etc. will be added later\n",
        "```\n",
        "\n",
        "\n",
        "* Execution specified by `ReadingService`\n",
        "  * Can adapt *Logical IR* to a backend *Execution Plan* \n",
        "\n",
        "```\n",
        "class ReadingService(ABC):\n",
        "  def initialize(self, dp: Datapipe) -> Datapipe:\n",
        "  def finalize(self) -> bool:\n",
        "  def iter_batches(self, state: Dict[str, Any]) -> RowBatchIterator:\n",
        "```\n",
        "```   \n",
        "class SimpleReadingService(ReadingService):\n",
        "class MultiprocessingReadingService(ReadingService):\n",
        "class OnboxDppReadingService(ReadingService):\n",
        "class DisaggDppReadingService(ReadingService):\n",
        "class RayDppReadingService(ReadingService):\n",
        "```\n",
        "* Connect business logic (specified by `Datapipe`) with execution engine (specified by `ReadingService`) and produce data through iteration by `Dataloader`\n",
        "\n",
        "```\n",
        "class Dataloader:\n",
        "  def __init__(self, datapipe: Datapipe, reading_service: ReadingService) -> None:\n",
        "  def reset(self) -> None:\n",
        "  def iter_batches(self) -> RowBatchIterator:\n",
        "  def __iter__(self) -> \"Dataloader\":\n",
        "  def __next__(self) -> RowBatch:\n",
        "  def state_dict(self) -> Dict[str, Any]:\n",
        "  def load_state_dict(self, state: Dict[str, Any]) -> bool:\n",
        "```\n",
        "\n",
        "### Note\n",
        "* Ray Dataset is a combination of all the 3 concepts above. For us, since we need to support multiple execution engines (e.g. DPP, multiprocessing, and potentially Sparse and Ray), separating `ReadingService` abstraction is useful. In addition, Pytorch already has `Dataloader` concept working with Dataset, this 3-component paradigm is reasonable.  \n",
        "* Ray DatasetPipeline is a parallelism mechanism that pipelining operations on batches. This is to overlapping data reading with training. We will ignore this use case in this doc. There is similar implementation in TorchRec. Will have follow-up design on enabling high efficient pipelining later. "
      ],
      "metadata": {
        "id": "jV-9LAZQkT5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install libs"
      ],
      "metadata": {
        "id": "GV-ab990sksg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw0zZ0LVthvj",
        "outputId": "402702d8-69d4-47d9-8990-afea15be9e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YprvdjGrC_e",
        "outputId": "c0e12bf9-a81b-46cd-9685-a38b61768470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow==7.0.0 in /usr/local/lib/python3.7/dist-packages (7.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ],
      "source": [
        "# MapArray.offsets need version 7.0.0\n",
        "!pip3 install pyarrow==7.0.0 pandas numpy \n",
        "!pip3 install torch --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "2B8yDR0Dspwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from enum import Enum\n",
        "from typing import (\n",
        "    Any, Collection, Dict, Iterable, Iterator, List, Optional, \n",
        "    Tuple, Type, Union, cast, Mapping, Callable, \n",
        ")\n",
        "from collections import UserDict\n",
        "\n",
        "def compute_offsets(input: Iterable, include_last: bool) -> List[int]:\n",
        "    offsets = list(itertools.accumulate([0] + input, lambda x, y: x + len(y)))\n",
        "    if not include_last:\n",
        "        return offsets[:-1]\n",
        "    return offsets\n",
        "\n",
        "\n",
        "def is_primitive(input: Any) -> bool:\n",
        "    PRIMITIVE = (int, str, bool, float)\n",
        "    if isinstance(input, PRIMITIVE):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "class StructType(Enum):\n",
        "    NON_LEAF = 0\n",
        "    LEAF_FLOAT = 1\n",
        "    LEAF_ID_LIST = 2\n",
        "    LEAF_ID_SCORE_MAP = 3\n",
        "\n",
        "\n",
        "def struct_type(input: Any) -> bool:\n",
        "    if type(input) == float:\n",
        "        return StructType.LEAF_FLOAT\n",
        "    if isinstance(input, dict) and (\n",
        "        len(input) == 0 or (type(next(iter(input.keys()))), type(next(iter(input.values())))) == (int, float)\n",
        "    ):\n",
        "        return StructType.LEAF_ID_SCORE_MAP\n",
        "    if type(input) == list and (len(input) == 0 or type(next(iter(input))) == int):\n",
        "        return StructType.LEAF_ID_LIST\n",
        "    return StructType.NON_LEAF\n",
        "    "
      ],
      "metadata": {
        "id": "qCCmKIZqrXQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datapipe: Data logical processor\n",
        "* Datapipe describes operations. It is STATELESS.\n",
        "The operation plan/IR can be compiled and reused (not implemented yet). Both training and inference logic can be described by Datapipe but should have different data source and operations (e.g. training specifies shuffle, while inference does not). \n",
        "\n",
        "* RowBatch: A batch of rows. "
      ],
      "metadata": {
        "id": "UaFK6TqKssws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datapipe Accessor\n",
        "\n",
        "Access data in specific format.\n"
      ],
      "metadata": {
        "id": "L8bfI-08I2Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow\n",
        "import torch\n",
        "from typing import TypeVar, Generic\n",
        "from abc import ABC, abstractmethod, abstractclassmethod\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class JaggedTensor:\n",
        "  offsets: Optional[torch.Tensor] = None\n",
        "  indices: Optional[torch.Tensor] = None\n",
        "  values: Optional[torch.Tensor] = None\n",
        "\n",
        "\n",
        "def to_torch(input: Union[dict, pyarrow.Array, JaggedTensor]) -> Union[dict, JaggedTensor]:\n",
        "  if type(input) == pyarrow.lib.FloatArray:\n",
        "    return JaggedTensor(\n",
        "        values=torch.from_numpy(input.to_numpy())\n",
        "    )\n",
        "  elif type(input) == pyarrow.lib.ListArray:\n",
        "    return JaggedTensor(\n",
        "        offsets=torch.from_numpy(input.offsets.to_numpy()),\n",
        "        indices=torch.from_numpy(input.values.to_numpy()),\n",
        "    )\n",
        "  elif type(input) == pyarrow.lib.MapArray:\n",
        "    return JaggedTensor(\n",
        "        offsets=torch.from_numpy(input.offsets.to_numpy()),\n",
        "        indices=torch.from_numpy(input.keys.to_numpy()),\n",
        "        values=torch.from_numpy(input.items.to_numpy()),\n",
        "    )\n",
        "  elif type(input) == JaggedTensor:\n",
        "    return input\n",
        "\n",
        "  output = {}\n",
        "  if isinstance(input, dict):\n",
        "    for key, val in input.items():\n",
        "      output[key] = to_torch(val)\n",
        "  return output\n",
        "\n",
        "\n",
        "\n",
        "Row = dict\n",
        "\n",
        "# collate operation such as to_torch() can be done by batch data OR\n",
        "# By building the operation as a part of execution plan by Datapipe so that it\n",
        "# will be conducted on server side.\n",
        "# Here we use RowBatch.to_torch() for demonstration purpose\n",
        "\n",
        "class RowBatch(dict):\n",
        "  def __init__(self, data, row_num: int) -> None:\n",
        "    self._row_num = row_num\n",
        "    super().__init__(data)\n",
        "    return\n",
        "\n",
        "  @property\n",
        "  def row_num(self) -> int:\n",
        "    return self._row_num\n",
        "\n",
        "  def to_torch(self, schema: Optional[dict] = None) -> dict:\n",
        "    if schema is not None:\n",
        "      raise NotImplementedError(\"not supporting customized schema\")\n",
        "    return to_torch(self)\n",
        "        \n",
        "\n",
        "# More robus design is needed for this class. \n",
        "# In this example, PyDictArrowDataAccessor constructs Arrow data from Python dict for demo purpose. \n",
        "class BatchAccessor(Enum):\n",
        "  NATIVE = \"native_batch_accessor\"\n",
        "  ARROW = \"pydict_arrow_batch_accessor\"\n",
        "  TORCH_TENSOR = \"torch_tensor_batch_accessor\"\n",
        "\n",
        "BATCH_ACCESSORS = {}\n",
        "\n",
        "def get_batch_accessor(name: BatchAccessor):\n",
        "  global BATCH_ACCESSORS\n",
        "  accessor = BATCH_ACCESSORS[name]()\n",
        "  return accessor\n",
        "\n",
        "def register_batch_accessor(name: BatchAccessor): \n",
        "  def register(cls):\n",
        "    print(f\"Registering Batch Accessor {name}: {cls}\")\n",
        "    global BATCH_ACCESSORS\n",
        "    BATCH_ACCESSORS[name] = cls\n",
        "    return cls\n",
        "  return register\n",
        "\n",
        "T = TypeVar(\"T\")\n",
        "class PyDictDataAccessor(ABC, Generic[T]):\n",
        "  @abstractmethod\n",
        "  def fetch_batch(self, data: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "    ...\n",
        "\n",
        "  @abstractclassmethod\n",
        "  def build_column_val(cls, input: List[Any]) -> T:\n",
        "    ...\n",
        "\n",
        "  @classmethod\n",
        "  def build_columns(cls, input: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "      assert len(input) > 0, f\"input {input} cannot have empty columns\"\n",
        "      col_values = []\n",
        "      row_num = block_size\n",
        "      for val in input.values():\n",
        "          if type(val) == list:\n",
        "              block_size = block_size or len(val)\n",
        "              end = min(start + block_size, len(val))\n",
        "              row_num = end - start\n",
        "              col_val = cls.build_column_val(val[start:end])\n",
        "          elif type(val) == dict:\n",
        "              col_val = cls.build_columns(val, block_size, start)\n",
        "              row_num = col_val.row_num\n",
        "          col_values.append(col_val)\n",
        "\n",
        "      return RowBatch(data=zip(input.keys(), col_values), row_num=row_num)\n",
        "\n",
        "# NOTE: Native mode does not work now!!!\n",
        "@register_batch_accessor(BatchAccessor.NATIVE)\n",
        "class PyDictNativeDataAccessor(PyDictDataAccessor[list]):\n",
        "  def fetch_batch(self, data: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "    return RowBatch(data, len(data))\n",
        "\n",
        "@register_batch_accessor(BatchAccessor.ARROW)\n",
        "class PyDictArrowDataAccessor(PyDictDataAccessor[pyarrow.Array]):\n",
        "  def fetch_batch(self, data: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "    return PyDictArrowDataAccessor.build_columns(input=data, block_size=block_size, start=start)\n",
        "\n",
        "  @classmethod\n",
        "  def build_column_val(cls, input: List[Any]) -> pyarrow.Array:\n",
        "      child_type = struct_type(next(iter(input)))\n",
        "      assert child_type != StructType.NON_LEAF, f\"Invalid input {input}\"\n",
        "      # List[List[int]]\n",
        "      if child_type == StructType.LEAF_ID_LIST:\n",
        "          col = pyarrow.ListArray.from_arrays(\n",
        "              offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "              values=pyarrow.array(list(itertools.chain(*input)), type=pyarrow.int64()),\n",
        "          )\n",
        "      # List[float]\n",
        "      elif child_type == StructType.LEAF_FLOAT:\n",
        "          col = pyarrow.array(input, type=pyarrow.float32())\n",
        "      # List[Dict[int, float]]\n",
        "      elif child_type == StructType.LEAF_ID_SCORE_MAP:\n",
        "          col = pyarrow.MapArray.from_arrays(\n",
        "              offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "              keys=pyarrow.array(list(itertools.chain(*[item.keys() for item in input])), type=pyarrow.int64()),\n",
        "              items=pyarrow.array(list(itertools.chain(*[item.values() for item in input])), type=pyarrow.float32()),\n",
        "          )\n",
        "      return col\n",
        "\n",
        "\n",
        "@register_batch_accessor(BatchAccessor.TORCH_TENSOR)\n",
        "class PyDictTorchTensorDataAccessor(PyDictDataAccessor[JaggedTensor]):\n",
        "  def fetch_batch(self, data: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "    return PyDictTorchTensorDataAccessor.build_columns(input=data, block_size=block_size, start=start)\n",
        "\n",
        "  @classmethod\n",
        "  def build_column_val(cls, input: List[Any]) -> JaggedTensor:\n",
        "      child_type = struct_type(next(iter(input)))\n",
        "      assert child_type != StructType.NON_LEAF, f\"Invalid input {input}\"\n",
        "      # List[List[int]]\n",
        "      if child_type == StructType.LEAF_ID_LIST:\n",
        "          col = JaggedTensor(\n",
        "              offsets=torch.tensor(compute_offsets(input, True), dtype=torch.int32),\n",
        "              indices=torch.tensor(list(itertools.chain(*input)), dtype=torch.int64),\n",
        "          )\n",
        "      # List[float]\n",
        "      elif child_type == StructType.LEAF_FLOAT:\n",
        "          col = JaggedTensor(\n",
        "              values=torch.tensor(input, dtype=torch.float32),\n",
        "          )\n",
        "      # List[Dict[int, float]]\n",
        "      elif child_type == StructType.LEAF_ID_SCORE_MAP:\n",
        "          col = JaggedTensor(\n",
        "              offsets=torch.tensor(compute_offsets(input, True), dtype=torch.int32),\n",
        "              indices=torch.tensor(list(itertools.chain(*[item.keys() for item in input])), dtype=torch.int64),\n",
        "              values=torch.tensor(list(itertools.chain(*[item.values() for item in input])), dtype=torch.float32),\n",
        "          )\n",
        "      return col\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIwifsz6I2uf",
        "outputId": "3a454e2d-7e0e-435d-fc92-fda59fbb61d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registering Batch Accessor BatchAccessor.NATIVE: <class '__main__.PyDictNativeDataAccessor'>\n",
            "Registering Batch Accessor BatchAccessor.ARROW: <class '__main__.PyDictArrowDataAccessor'>\n",
            "Registering Batch Accessor BatchAccessor.TORCH_TENSOR: <class '__main__.PyDictTorchTensorDataAccessor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RowBatchIterator(Iterator[RowBatch]):\n",
        "  def __init__(self, fetch_fn: Callable[[Dict[str, Any]], RowBatch], states: Dict[str, Any]) -> None:\n",
        "    self._fetch_fn = fetch_fn\n",
        "    self._orig_states = states\n",
        "    self._states = states\n",
        "\n",
        "  def __iter__(self) -> \"RowBatchIterator\": \n",
        "    # reset iterator\n",
        "    self._states = self._orig_states\n",
        "    return self\n",
        "\n",
        "  def __next__(self) -> RowBatch:\n",
        "    data, self._states = self._fetch_fn(self._states)\n",
        "    return data\n",
        "\n",
        "\n",
        "class ShuffleSpec(Enum):\n",
        "  NO_SHUFFLE = 0\n",
        "  FULL_SHUFFLE = 1\n",
        "\n",
        "\n",
        "class Datapipe(ABC):\n",
        "  def __init__(self) -> None:\n",
        "    self._batch_size = 3\n",
        "    self._batch_accessor = BatchAccessor.NATIVE\n",
        "\n",
        "  # Commenting out all plan building logic for now.\n",
        "  def shuffle(self, shuffle_spec: ShuffleSpec) -> \"Datapipe\":\n",
        "    self._shuffle_spec = shuffle_spec\n",
        "    # self._plan = PlanBuilder.addShuffle(shuffle_spec)\n",
        "    return self\n",
        "\n",
        "  def repeat(self, num: int) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addRepeat(num)\n",
        "    self._total_passes = num\n",
        "    return self\n",
        "\n",
        "  def sort(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addSort(fn)\n",
        "    return self\n",
        "\n",
        "  def map_batches(\n",
        "      self, \n",
        "      fn: Callable[[RowBatch], RowBatch], \n",
        "      batch_size: Optional[int] = None,\n",
        "      batch_accessor: Optional[BatchAccessor] = None,\n",
        "    ) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addMapBatches(fn, batch_size)\n",
        "    self._map_batch_fn = fn\n",
        "    self._batch_size = batch_size or self._batch_size\n",
        "    self._batch_accessor = batch_accessor or self._batch_accessor\n",
        "    return self\n",
        "\n",
        "  # can be implemented with map_batches()\n",
        "  def filter(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addFilter(fn)\n",
        "    return self\n",
        "\n",
        "  # can be implemented with map_batches()\n",
        "  def map(self, fn: Callable[[Row], Row]) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addMap(fn)\n",
        "    return self\n",
        "\n",
        "  def rebatch(self, batch_size: int) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.rebatch(batch_size)\n",
        "    self._batch_size = batch_size\n",
        "    return self\n",
        "\n",
        "  def change_batch_accessor(self, batch_accessor: BatchAccessor) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.batchFormat(batch_format)\n",
        "    self._batch_accessor = batch_accessor\n",
        "    return self\n",
        "\n",
        "  # aggregation functions like max, mean, min, sum, etc. will be added later\n",
        "\n",
        "  @property\n",
        "  def batch_size(self) -> int:\n",
        "    return self._batch_size\n",
        "\n",
        "# IterDatapipe -- iterate over data source to produce data. \n",
        "# Used in training together with Dataloader and ReadingService\n",
        "class IterDatapipe(Datapipe):\n",
        "  @abstractmethod\n",
        "  def iter_batches(self, batch_size: int, start: int = 0, stride: int = 1) -> RowBatchIterator:\n",
        "    pass\n",
        "\n",
        "\n",
        "class PyDictDatapipe(IterDatapipe): \n",
        "  def __init__(\n",
        "      self, data: Dict[str, Any], \n",
        "      batch_size: int = 3, \n",
        "      batch_accessor: BatchAccessor = BatchAccessor.NATIVE,\n",
        "  ) -> None:\n",
        "    super().__init__()\n",
        "    self._data = data\n",
        "    self._batch_size = batch_size\n",
        "    self._batch_accessor = batch_accessor\n",
        "\n",
        "  def fetch_batch(self, states: Dict[str, Any]) -> Tuple[RowBatch, Dict[str, Any]]:\n",
        "    # The current implementation is for demo purpose. \n",
        "    # Real implementation should be based on executing self._plan rather than calling specific functions\n",
        "    batch = get_batch_accessor(self._batch_accessor).fetch_batch(self._data, states[\"batch_size\"], states[\"start\"])\n",
        "    if hasattr(self, \"_map_batch_fn\"):\n",
        "      batch = self._map_batch_fn(batch)\n",
        "    states[\"start\"] += states[\"batch_size\"] * states[\"stride\"]\n",
        "    return (batch, states)\n",
        "\n",
        "  def iter_batches(self, batch_size: int, start: int = 0, stride: int = 1) -> RowBatchIterator:\n",
        "    return RowBatchIterator(self.fetch_batch, {\"start\": start, \"batch_size\": batch_size, \"stride\": stride})\n",
        "\n",
        "# class HiveDatapipe(IterDatapipe):\n",
        "#   def __init__(self, hive_specs: Dict[str, Any], block_size: int, compute_adaptor) -> None: \n",
        "\n",
        "# CallableDatapipe -- Apply transformation on data. \n",
        "# Action in reactive way (need to be called with provided data). Userd in inference\n",
        "class CallableDatapipe(Datapipe):\n",
        "  def __call__(self, data):\n",
        "    raise NotImplementedError(\"not supporting customized schema\")\n",
        "\n",
        "class SchemaDatapipe(CallableDatapipe):\n",
        "  def __init__(self, schema: Dict[str, Any], batch_accessor: str) -> None:\n",
        "    super().__init__()\n",
        "    self._schema = schema\n",
        "    self._batch_accessor = batch_accessor\n",
        "\n",
        "  def conversion(self, data) -> RowBatch:\n",
        "    # The current implementation is for demo purpose. \n",
        "    # Real implementation should be based on executing self._plan rather than calling specific functions\n",
        "    batch = get_batch_accessor(self._batch_accessor).fetch_batch(data, None, 0)\n",
        "    if hasattr(self, \"_map_batch_fn\"):\n",
        "      batch = self._map_batch_fn(batch)\n",
        "    return batch\n",
        "\n",
        "  def __call__(self, data) -> RowBatch: \n",
        "    return self.conversion(data)"
      ],
      "metadata": {
        "id": "VuQFXjQWrKKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Data"
      ],
      "metadata": {
        "id": "JpvPivsgZuwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data={\n",
        "    \"float_features\": {\n",
        "        \"f1\": [1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
        "        \"f2\": [2.0, 2.1, 2.2, 2.3, 2.4, 2.5],\n",
        "    },\n",
        "    \"id_list_features\": {\n",
        "        \"id1\": [[111, 112], [121], [131, 132], [], [151], [161]],\n",
        "        \"id2\": [[], [221], [222], [223, 224], [225], []],\n",
        "        \"id3\": [[311], [], [331], [341], [351], [361]],\n",
        "    },\n",
        "    \"id_score_list_features\": {\n",
        "        \"ids1\": [{411: 0.1}, {421: 0.2}, {431: 0.7}, {}, {451: 0.4, 452: 0.1}, {}],\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "APQLSL-gs3ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_ACCESSORS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAOs16GwbGFQ",
        "outputId": "7e8b8230-d322-4305-d9b4-8c07557351e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{<BatchAccessor.ARROW: 'pydict_arrow_batch_accessor'>: __main__.PyDictArrowDataAccessor,\n",
              " <BatchAccessor.NATIVE: 'native_batch_accessor'>: __main__.PyDictNativeDataAccessor}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arrow_dp = PyDictDatapipe(\n",
        "    data=data,\n",
        "    batch_size=4,\n",
        "    batch_accessor=BatchAccessor.ARROW,\n",
        ")\n",
        "# test single iter\n",
        "rb = next(iter(arrow_dp.iter_batches(batch_size=4, start=0, stride=1)))\n",
        "type(rb)\n",
        "rb.row_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihOdG6wptOPC",
        "outputId": "1d7e44a7-12c8-45eb-e220-daf561547d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.compute as F\n",
        "\n",
        "print(rb[\"float_features\"][\"f1\"])\n",
        "print(F.ln(F.add(rb[\"float_features\"][\"f1\"], 1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etchmkzns8bR",
        "outputId": "df962523-8888-46b2-f04c-747aea34b3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  1,\n",
            "  1.1,\n",
            "  1.2,\n",
            "  1.3\n",
            "]\n",
            "[\n",
            "  0.6931472,\n",
            "  0.7419373,\n",
            "  0.7884574,\n",
            "  0.8329091\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test iter_batches\n",
        "for batch in arrow_dp.iter_batches(batch_size=5):\n",
        "  print(batch[\"float_features\"][\"f1\"])\n",
        "  print(batch.row_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PQykZIjzChC",
        "outputId": "44d8e48f-21ed-4e3f-ff44-8a8b54d7088b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  1,\n",
            "  1.1,\n",
            "  1.2,\n",
            "  1.3,\n",
            "  1.4\n",
            "]\n",
            "5\n",
            "[\n",
            "  1.5\n",
            "]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor DP\n",
        "tensor_dp = PyDictDatapipe(\n",
        "    data=data,\n",
        "    batch_size=4,\n",
        "    batch_accessor=BatchAccessor.TORCH_TENSOR,\n",
        ")"
      ],
      "metadata": {
        "id": "sFu3quVCAcWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test single iter\n",
        "rb = next(iter(tensor_dp.iter_batches(batch_size=4, start=0, stride=1)))\n",
        "type(rb)\n",
        "rb.row_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1GxKaBmAndL",
        "outputId": "c030fd17-76ad-4a77-f0f4-522ea1bb940e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in tensor_dp.iter_batches(batch_size=5):\n",
        "  print(batch[\"float_features\"][\"f1\"])\n",
        "  print(batch.row_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAmlJY8FAs7m",
        "outputId": "0f3eb24a-2f21-4382-b6d0-775652d8bcdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JaggedTensor(offsets=None, indices=None, values=tensor([1.0000, 1.1000, 1.2000, 1.3000, 1.4000]))\n",
            "5\n",
            "JaggedTensor(offsets=None, indices=None, values=tensor([1.5000]))\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReadingService\n",
        "Specify the data reading execution engine. It should be STATELESS\n",
        "\n",
        "\n",
        "* SimpleReadingService\n",
        "* DisaggDppReadingService\n",
        "* OnboxDppReadingService\n",
        "* OnboxMultiprocessReadingService"
      ],
      "metadata": {
        "id": "w1n7y89owsf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReadingService(ABC):\n",
        "  def __init__(self) -> None:\n",
        "    \n",
        "    return\n",
        "\n",
        "  def initialize(self, dp: IterDatapipe) -> IterDatapipe:\n",
        "    raise NotImplementedError(\"Not implement initialize()\")\n",
        "\n",
        "  def finalize(self) -> bool:\n",
        "    return True\n",
        "\n",
        "  def iter_batches(self, dp: IterDatapipe, state: Dict[str, Any]) -> RowBatchIterator:\n",
        "    raise NotImplementedError(\"Not implement read_batch()\")\n",
        "\n",
        "\n",
        "class SimpleReadingService(ReadingService):\n",
        "  def initialize(self, dp: Datapipe) -> IterDatapipe:\n",
        "    return dp\n",
        "\n",
        "  def iter_batches(self, dp: IterDatapipe, state: Dict[str, Any]) -> RowBatchIterator:\n",
        "    yield from dp.iter_batches(batch_size=dp.batch_size, start=state[\"start\"], stride=1)\n"
      ],
      "metadata": {
        "id": "zWEy4RyCwqWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader\n",
        "* Take Datapipe (describing business logic) and ReadingService (specifying execution engine) to produce data batches. \n",
        "* Dataloader is STATEFUL\n",
        "* Provide checkpointing interface"
      ],
      "metadata": {
        "id": "EMjCqevIx_GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataloader:\n",
        "  def __init__(self, datapipe: Datapipe, reading_service: ReadingService) -> None:\n",
        "    self._dp = datapipe\n",
        "    self._rs = reading_service\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self) -> None:\n",
        "    self._states = {\n",
        "        \"start\": 0,\n",
        "    }\n",
        "    self._dp = self._rs.initialize(self._dp)\n",
        "\n",
        "  def iter_batches(self) -> RowBatchIterator:\n",
        "    # TODO: need to update self._states appropriately\n",
        "    yield from self._rs.iter_batches(self._dp, self._states)\n",
        "\n",
        "  def __iter__(self) -> \"Dataloader\":\n",
        "    self.reset()\n",
        "    return self\n",
        "\n",
        "  def __next__(self) -> RowBatch:\n",
        "    return next(self.iter_batches())\n",
        "\n",
        "  def state_dict(self) -> Dict[str, Any]:\n",
        "    return self._states\n",
        "\n",
        "  def load_state_dict(self, states: Dict[str, Any]) -> bool:\n",
        "    self._states = states\n",
        "    return True"
      ],
      "metadata": {
        "id": "GmJeviKSyQqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "### Arrow Preproc"
      ],
      "metadata": {
        "id": "jhzqtpmrbEc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Arrow Preproc\n",
        "import pyarrow.compute as F\n",
        "\n",
        "def collate(rows: RowBatch) -> RowBatch:\n",
        "  return rows.to_torch()\n",
        "\n",
        "# could be just Row\n",
        "def arrow_preproc(rows: RowBatch) -> RowBatch:\n",
        "    rows[\"float_features\"][\"f1\"] = F.ln(F.add(rows[\"float_features\"][\"f1\"], 3))\n",
        "    # can collate in preproc. Or can collate in training loop.\n",
        "    output = collate(rows)\n",
        "    return output"
      ],
      "metadata": {
        "id": "lW43mh7GJCsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arrow_dp = PyDictDatapipe(data=data)\n",
        "# dp = dp.repeat(num=2)\n",
        "# dp = dp.shuffle(shuffle_mode=ShuffleMode.SHAFFLE_ALL)\n",
        "arrow_dp = arrow_dp.map_batches(arrow_preproc, batch_size=5, batch_accessor=BatchAccessor.ARROW)\n",
        "\n",
        "rs = SimpleReadingService()\n",
        "\n",
        "dl = Dataloader(arrow_dp, rs)\n",
        "\n",
        "for idx, batch in enumerate(dl.iter_batches()):\n",
        "  # print(f\"==Batch {idx}, size {batch.row_num}==\")\n",
        "  # input = batch.to_torch()\n",
        "  print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LRn17KihduG",
        "outputId": "10aad853-5057-412a-91f9-63983e950a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'float_features': {'f1': JaggedTensor(offsets=None, indices=None, values=tensor([1.3863, 1.4110, 1.4351, 1.4586, 1.4816])), 'f2': JaggedTensor(offsets=None, indices=None, values=tensor([2.0000, 2.1000, 2.2000, 2.3000, 2.4000]))}, 'id_list_features': {'id1': JaggedTensor(offsets=tensor([0, 2, 3, 5, 5, 6], dtype=torch.int32), indices=tensor([111, 112, 121, 131, 132, 151]), values=None), 'id2': JaggedTensor(offsets=tensor([0, 0, 1, 2, 4, 5], dtype=torch.int32), indices=tensor([221, 222, 223, 224, 225]), values=None), 'id3': JaggedTensor(offsets=tensor([0, 1, 1, 2, 3, 4], dtype=torch.int32), indices=tensor([311, 331, 341, 351]), values=None)}, 'id_score_list_features': {'ids1': JaggedTensor(offsets=tensor([0, 1, 2, 3, 3, 5], dtype=torch.int32), indices=tensor([411, 421, 431, 451, 452]), values=tensor([0.1000, 0.2000, 0.7000, 0.4000, 0.1000]))}}\n",
            "{'float_features': {'f1': JaggedTensor(offsets=None, indices=None, values=tensor([1.5041])), 'f2': JaggedTensor(offsets=None, indices=None, values=tensor([2.5000]))}, 'id_list_features': {'id1': JaggedTensor(offsets=tensor([0, 1], dtype=torch.int32), indices=tensor([161]), values=None), 'id2': JaggedTensor(offsets=tensor([0, 0], dtype=torch.int32), indices=tensor([], dtype=torch.int64), values=None), 'id3': JaggedTensor(offsets=tensor([0, 1], dtype=torch.int32), indices=tensor([361]), values=None)}, 'id_score_list_features': {'ids1': JaggedTensor(offsets=tensor([0, 0], dtype=torch.int32), indices=tensor([], dtype=torch.int64), values=tensor([]))}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Preproc"
      ],
      "metadata": {
        "id": "ZVl8Z4DtC6nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# could be just Row\n",
        "def tensor_preproc(rows: RowBatch) -> RowBatch:\n",
        "    rows[\"float_features\"][\"f1\"].values = rows[\"float_features\"][\"f1\"].values.add(3)\n",
        "    rows[\"id_list_features\"][\"id1\"].indices = torch.remainder(rows[\"id_list_features\"][\"id1\"].indices, 100)\n",
        "    # can collate in preproc. Or can collate in training loop.\n",
        "    output = collate(rows)\n",
        "    return output\n",
        "\n",
        "tensor_dp = PyDictDatapipe(data=data)\n",
        "# dp = dp.repeat(num=2)\n",
        "# dp = dp.shuffle(shuffle_mode=ShuffleMode.SHAFFLE_ALL)\n",
        "tensor_dp = tensor_dp.map_batches(tensor_preproc, batch_size=5, batch_accessor=BatchAccessor.TORCH_TENSOR)\n",
        "\n",
        "rs = SimpleReadingService()\n",
        "\n",
        "dl = Dataloader(tensor_dp, rs)\n",
        "\n",
        "for idx, batch in enumerate(dl.iter_batches()):\n",
        "  # print(f\"==Batch {idx}, size {batch.row_num}==\")\n",
        "  # input = batch.to_torch()\n",
        "  print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z19nbIyC0mA",
        "outputId": "c6b90f58-18a4-451b-d223-2a6a82a88c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'float_features': {'f1': JaggedTensor(offsets=None, indices=None, values=tensor([4.0000, 4.1000, 4.2000, 4.3000, 4.4000])), 'f2': JaggedTensor(offsets=None, indices=None, values=tensor([2.0000, 2.1000, 2.2000, 2.3000, 2.4000]))}, 'id_list_features': {'id1': JaggedTensor(offsets=tensor([0, 2, 3, 5, 5, 6], dtype=torch.int32), indices=tensor([11, 12, 21, 31, 32, 51]), values=None), 'id2': JaggedTensor(offsets=tensor([0, 0, 1, 2, 4, 5], dtype=torch.int32), indices=tensor([221, 222, 223, 224, 225]), values=None), 'id3': JaggedTensor(offsets=tensor([0, 1, 1, 2, 3, 4], dtype=torch.int32), indices=tensor([311, 331, 341, 351]), values=None)}, 'id_score_list_features': {'ids1': JaggedTensor(offsets=tensor([0, 1, 2, 3, 3, 5], dtype=torch.int32), indices=tensor([411, 421, 431, 451, 452]), values=tensor([0.1000, 0.2000, 0.7000, 0.4000, 0.1000]))}}\n",
            "{'float_features': {'f1': JaggedTensor(offsets=None, indices=None, values=tensor([4.5000])), 'f2': JaggedTensor(offsets=None, indices=None, values=tensor([2.5000]))}, 'id_list_features': {'id1': JaggedTensor(offsets=tensor([0, 1], dtype=torch.int32), indices=tensor([61]), values=None), 'id2': JaggedTensor(offsets=tensor([0, 0], dtype=torch.int32), indices=tensor([], dtype=torch.int64), values=None), 'id3': JaggedTensor(offsets=tensor([0, 1], dtype=torch.int32), indices=tensor([361]), values=None)}, 'id_score_list_features': {'ids1': JaggedTensor(offsets=tensor([0, 0], dtype=torch.int32), indices=tensor([], dtype=torch.int64), values=tensor([]))}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MaM0SUoVjrws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infernece"
      ],
      "metadata": {
        "id": "nr66zYTmibkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SCHEMA = (\n",
        "    (\"float_features\", Dict[str, List[float]]),\n",
        "    (\"float_features\", Dict[str, List[List[int]]]),\n",
        "    (\"id_score_list_features\", Dict[str, List[Dict[int, float]]]),\n",
        ")\n",
        "arrow_dp = SchemaDatapipe(schema=INPUT_SCHEMA, batch_accessor=BatchAccessor.ARROW)\n",
        "darrow_dpp = arrow_dp.map_batches(arrow_preproc)\n",
        "pred = arrow_dp(data)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfUnQpBsidOO",
        "outputId": "c3ea9457-9eb6-470f-c8df-12dc4d17ff4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'float_features': {'f1': JaggedTensor(offsets=None, indices=None, values=tensor([1.3863, 1.4110, 1.4351, 1.4586, 1.4816, 1.5041])),\n",
              "  'f2': JaggedTensor(offsets=None, indices=None, values=tensor([2.0000, 2.1000, 2.2000, 2.3000, 2.4000, 2.5000]))},\n",
              " 'id_list_features': {'id1': JaggedTensor(offsets=tensor([0, 2, 3, 5, 5, 6, 7], dtype=torch.int32), indices=tensor([111, 112, 121, 131, 132, 151, 161]), values=None),\n",
              "  'id2': JaggedTensor(offsets=tensor([0, 0, 1, 2, 4, 5, 5], dtype=torch.int32), indices=tensor([221, 222, 223, 224, 225]), values=None),\n",
              "  'id3': JaggedTensor(offsets=tensor([0, 1, 1, 2, 3, 4, 5], dtype=torch.int32), indices=tensor([311, 331, 341, 351, 361]), values=None)},\n",
              " 'id_score_list_features': {'ids1': JaggedTensor(offsets=tensor([0, 1, 2, 3, 3, 5, 5], dtype=torch.int32), indices=tensor([411, 421, 431, 451, 452]), values=tensor([0.1000, 0.2000, 0.7000, 0.4000, 0.1000]))}}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_dp = SchemaDatapipe(schema=INPUT_SCHEMA, batch_accessor=BatchAccessor.TORCH_TENSOR)\n",
        "tensor_dp = tensor_dp.map_batches(tensor_preproc)\n",
        "pred = tensor_dp(data)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37Uf5AgtD6Ug",
        "outputId": "d1877cad-7fe0-41ce-d767-31c9072fd5a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'float_features': {'f1': JaggedTensor(offsets=None, indices=None, values=tensor([4.0000, 4.1000, 4.2000, 4.3000, 4.4000, 4.5000])),\n",
              "  'f2': JaggedTensor(offsets=None, indices=None, values=tensor([2.0000, 2.1000, 2.2000, 2.3000, 2.4000, 2.5000]))},\n",
              " 'id_list_features': {'id1': JaggedTensor(offsets=tensor([0, 2, 3, 5, 5, 6, 7], dtype=torch.int32), indices=tensor([11, 12, 21, 31, 32, 51, 61]), values=None),\n",
              "  'id2': JaggedTensor(offsets=tensor([0, 0, 1, 2, 4, 5, 5], dtype=torch.int32), indices=tensor([221, 222, 223, 224, 225]), values=None),\n",
              "  'id3': JaggedTensor(offsets=tensor([0, 1, 1, 2, 3, 4, 5], dtype=torch.int32), indices=tensor([311, 331, 341, 351, 361]), values=None)},\n",
              " 'id_score_list_features': {'ids1': JaggedTensor(offsets=tensor([0, 1, 2, 3, 3, 5, 5], dtype=torch.int32), indices=tensor([411, 421, 431, 451, 452]), values=tensor([0.1000, 0.2000, 0.7000, 0.4000, 0.1000]))}}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archived"
      ],
      "metadata": {
        "id": "vwC-VypRDYZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class Table(ABC, dict):\n",
        "    def __init__(self, data: Dict[str, Any], block_size: int) -> None: # , start: int = 0, stride: int = 1) -> None:\n",
        "        self._data = data\n",
        "        self._block_size = block_size\n",
        "        # self._start = start\n",
        "        # self._stride = stride\n",
        "\n",
        "    # def __iter__(self) -> RowBatchIterator:\n",
        "    #     return RowBatchIterator(self, self._start, self._block_size, self._stride)\n",
        "\n",
        "    # TODO: wonder if we should have iterator method in Table?\n",
        "    #       considering Table is an \"accessors of data\", but not necessary storing the real data, having it seems reasonable\n",
        "    def iter_batches(self, batch_size: int, start: int = 0, stride: int = 1) -> RowBatchIterator:\n",
        "      return RowBatchIterator(self, start=start, batch_size=batch_size, stride=stride)\n",
        "\n",
        "    # TODO: this fucntion will be called by RowBatchIterator.__next__(). Seems a bit cyclic...\n",
        "    @abstractmethod\n",
        "    def fetch_batch(self, start: int, batch_size: int) -> RowBatch:\n",
        "      ...\n",
        "\n",
        "\n",
        "class ArrowTable(Table):\n",
        "    # def __next__(self) -> RowBatch:\n",
        "    #     columns = ArrowTable.build_columns(self._data, self._block_size, self._start)\n",
        "    #     self._start += self._block_size * self._stride\n",
        "    #     return columns\n",
        "    \n",
        "    def fetch_batch(self, start: int, batch_size: int) -> RowBatch:\n",
        "      return ArrowTable.build_columns(self._data, batch_size, start)\n",
        "\n",
        "    @staticmethod\n",
        "    def build_column_val(input: List[Any]) -> pyarrow.Array:\n",
        "        child_type = struct_type(next(iter(input)))\n",
        "        assert child_type != StructType.NON_LEAF, f\"Invalid input {input}\"\n",
        "        # List[List[int]]\n",
        "        if child_type == StructType.LEAF_ID_LIST:\n",
        "            col = pyarrow.ListArray.from_arrays(\n",
        "                offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "                values=list(itertools.chain(*input)),\n",
        "            )\n",
        "        # List[float]\n",
        "        elif child_type == StructType.LEAF_FLOAT:\n",
        "            col = pyarrow.array(input, type=pyarrow.float32())\n",
        "        # List[Dict[int, float]]\n",
        "        elif child_type == StructType.LEAF_ID_SCORE_MAP:\n",
        "            col = pyarrow.MapArray.from_arrays(\n",
        "                offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "                keys=list(itertools.chain(*[item.keys() for item in input])),\n",
        "                items=list(itertools.chain(*[item.values() for item in input])),\n",
        "            )\n",
        "        return col\n",
        "\n",
        "    @staticmethod\n",
        "    def build_columns(input: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "        assert len(input) > 0, f\"input {input} cannot have empty columns\"\n",
        "        col_values = []\n",
        "        row_num = block_size\n",
        "        for val in input.values():\n",
        "            if type(val) == list:\n",
        "                block_size = block_size or len(val)\n",
        "                end = min(start + block_size, len(val))\n",
        "                row_num = end - start\n",
        "                col_val = ArrowTable.build_column_val(val[start:end])\n",
        "            elif type(val) == dict:\n",
        "                col_val = ArrowTable.build_columns(val, block_size, start)\n",
        "                row_num = col_val.row_num\n",
        "            col_values.append(col_val)\n",
        "\n",
        "        return RowBatch(data=zip(input.keys(), col_values), row_num=row_num)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_pydict(input: Dict[str, Any], block_size: int) -> \"ArrowTable\":\n",
        "        return ArrowTable(data=input, block_size=block_size)"
      ],
      "metadata": {
        "id": "PcvYtV_CDXf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}