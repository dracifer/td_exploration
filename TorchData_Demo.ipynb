{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TorchData Demo",
      "provenance": [],
      "collapsed_sections": [
        "vwC-VypRDYZM"
      ],
      "authorship_tag": "ABX9TyOqri+Y+Rsg9mqTFXFmxdgA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dracifer/td_exploration/blob/main/TorchData_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Option 1 -- Cohesive with current Dataloader V2 design:\n",
        "* Business logic expressed by `Datapipe` (or conventional `Dataset`)\n",
        "  * Can be traced to a backend independent execution plan/graph -- *Logical IR*\n",
        "  * Function calls like `shuffle` and `map_batches` accumulately build the plan rother than execute immediately. Actual data loading and processing is triggered by an \"execution\" call. \n",
        "  * Datapipe can be used to compose plans for both training and inference. In certain cases (e.g. eager serving mode), Datapipe is not needed. \n",
        "```\n",
        "class Datapipe:\n",
        "    def shuffle(self, shuffle_spec: ShuffleSpec) -> \"Datapipe\":\n",
        "    def repeat(self, num: int) -> \"Datapipe\":\n",
        "    def sort(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    def map_batches(self, fn: Callable[[RowBatch], RowBatch], batch_size: Optional[int] = None) -> \"Datapipe\":\n",
        "    def filter(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    # aggregation functions like max, mean, min, sum, etc. will be added later\n",
        "```\n",
        "\n",
        "\n",
        "* Execution specified by `ReadingService`\n",
        "  * Can adapt *Logical IR* to a backend *Execution Plan* \n",
        "\n",
        "```\n",
        "class ReadingService(ABC):\n",
        "  def initialize(self, dp: Datapipe) -> Datapipe:\n",
        "  def finalize(self) -> bool:\n",
        "  def iter_batches(self, state: Dict[str, Any]) -> RowBatchIterator:\n",
        "```\n",
        "```   \n",
        "class SimpleReadingService(ReadingService):\n",
        "class MultiprocessingReadingService(ReadingService):\n",
        "class OnboxDppReadingService(ReadingService):\n",
        "class DisaggDppReadingService(ReadingService):\n",
        "class RayDppReadingService(ReadingService):\n",
        "```\n",
        "* Connect business logic (specified by `Datapipe`) with execution engine (specified by `ReadingService`) and produce data through iteration by `Dataloader`\n",
        "\n",
        "```\n",
        "class Dataloader:\n",
        "  def __init__(self, datapipe: Datapipe, reading_service: ReadingService) -> None:\n",
        "  def reset(self) -> None:\n",
        "  def iter_batches(self) -> RowBatchIterator:\n",
        "  def __iter__(self) -> \"Dataloader\":\n",
        "  def __next__(self) -> RowBatch:\n",
        "  def state_dict(self) -> Dict[str, Any]:\n",
        "  def load_state_dict(self, state: Dict[str, Any]) -> bool:\n",
        "```\n",
        "\n",
        "### Note\n",
        "* Ray Dataset is a combination of all the 3 concepts above. For us, since we need to support multiple execution engines (e.g. DPP, multiprocessing, and potentially Sparse and Ray), separating `ReadingService` abstraction is useful. In addition, Pytorch already has `Dataloader` concept working with Dataset, this 3-component paradigm is reasonable.  \n",
        "* Ray DatasetPipeline is a parallelism mechanism that pipelining operations on batches. This is to overlapping data reading with training. We will ignore this use case in this doc. There is similar implementation in TorchRec. Will have follow-up design on enabling high efficient pipelining later. "
      ],
      "metadata": {
        "id": "jV-9LAZQkT5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## install libs"
      ],
      "metadata": {
        "id": "GV-ab990sksg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw0zZ0LVthvj",
        "outputId": "402702d8-69d4-47d9-8990-afea15be9e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "2YprvdjGrC_e",
        "outputId": "e6867d16-c4ae-41e2-9460-406a9afd8aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow==7.0.0\n",
            "  Downloading pyarrow-7.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.7 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Installing collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 6.0.1\n",
            "    Uninstalling pyarrow-6.0.1:\n",
            "      Successfully uninstalled pyarrow-6.0.1\n",
            "Successfully installed pyarrow-7.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# MapArray.offsets need version 7.0.0\n",
        "!pip3 install pyarrow==7.0.0 pandas numpy \n",
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "2B8yDR0Dspwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from enum import Enum\n",
        "from typing import Any, Collection, Dict, Iterable, Iterator, List, Optional, Tuple, Type, Union, cast, Mapping, Callable\n",
        "from collections import UserDict\n",
        "\n",
        "def compute_offsets(input: Iterable, include_last: bool) -> List[int]:\n",
        "    offsets = list(itertools.accumulate([0] + input, lambda x, y: x + len(y)))\n",
        "    if not include_last:\n",
        "        return offsets[:-1]\n",
        "    return offsets\n",
        "\n",
        "\n",
        "def is_primitive(input: Any) -> bool:\n",
        "    PRIMITIVE = (int, str, bool, float)\n",
        "    if isinstance(input, PRIMITIVE):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "class StructType(Enum):\n",
        "    NON_LEAF = 0\n",
        "    LEAF_FLOAT = 1\n",
        "    LEAF_ID_LIST = 2\n",
        "    LEAF_ID_SCORE_MAP = 3\n",
        "\n",
        "\n",
        "def struct_type(input: Any) -> bool:\n",
        "    if type(input) == float:\n",
        "        return StructType.LEAF_FLOAT\n",
        "    if isinstance(input, dict) and (\n",
        "        len(input) == 0 or (type(next(iter(input.keys()))), type(next(iter(input.values())))) == (int, float)\n",
        "    ):\n",
        "        return StructType.LEAF_ID_SCORE_MAP\n",
        "    if type(input) == list and (len(input) == 0 or type(next(iter(input))) == int):\n",
        "        return StructType.LEAF_ID_LIST\n",
        "    return StructType.NON_LEAF\n",
        "    "
      ],
      "metadata": {
        "id": "qCCmKIZqrXQF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datapipe: Data logical processor\n",
        "* Datapipe describes operations. It is STATELESS.\n",
        "The operation plan/IR can be compiled and reused (not implemented yet). Both training and inference logic can be described by Datapipe but should have different data source and operations (e.g. training specifies shuffle, while inference does not). \n",
        "\n",
        "* RowBatch: A batch of rows. "
      ],
      "metadata": {
        "id": "UaFK6TqKssws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datapipe Accessor\n",
        "\n",
        "Access data in specific format.\n"
      ],
      "metadata": {
        "id": "L8bfI-08I2Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow\n",
        "import torch\n",
        "from typing import TypeVar\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "def to_torch(input: Union[dict, pyarrow.Array]) -> Union[dict, tuple]:\n",
        "  if type(input) == pyarrow.lib.FloatArray:\n",
        "    return (\n",
        "        torch.ones(len(input), dtype=torch.int8),\n",
        "        torch.from_numpy(input.to_numpy()),\n",
        "    )\n",
        "  elif type(input) == pyarrow.lib.ListArray:\n",
        "    return (\n",
        "        torch.from_numpy(input.offsets.to_numpy()),\n",
        "        torch.from_numpy(input.values.to_numpy()),\n",
        "    )\n",
        "  elif type(input) == pyarrow.lib.MapArray:\n",
        "    return (\n",
        "        torch.from_numpy(input.offsets.to_numpy()),\n",
        "        torch.from_numpy(input.keys.to_numpy()),\n",
        "        torch.from_numpy(input.items.to_numpy()),\n",
        "    )\n",
        "\n",
        "  output = {}\n",
        "  if isinstance(input, dict):\n",
        "    for key, val in input.items():\n",
        "      output[key] = to_torch(val)\n",
        "  return output\n",
        "\n",
        "\n",
        "Row = dict\n",
        "\n",
        "# collate operation such as to_torch() can be done by batch data OR\n",
        "# By building the operation as a part of execution plan by Datapipe so that it\n",
        "# will be conducted on server side.\n",
        "# Here we use RowBatch.to_torch() for demonstration purpose\n",
        "class RowBatch(dict):\n",
        "  def __init__(self, data, row_num: int):\n",
        "    self._row_num = row_num\n",
        "    super().__init__(data)\n",
        "    return\n",
        "\n",
        "  @property\n",
        "  def row_num(self):\n",
        "    return self._row_num\n",
        "\n",
        "  def to_torch(self, schema: Optional[dict] = None) -> dict:\n",
        "    if schema is not None:\n",
        "      raise NotImplementedError(\"not supporting customized schema\")\n",
        "    return to_torch(self)\n",
        "        \n",
        "\n",
        "# More robus design is needed for this class. \n",
        "# In this example, PyDictArrowDataAccessor constructs Arrow data from Python dict for demo purpose. \n",
        "BATCH_ACCESSORS = {}\n",
        "DEFAULT_BATCH_ACCESSOR = \"native_batch_accessor\"\n",
        "\n",
        "def get_batch_accessor(name: str):\n",
        "  global BATCH_ACCESSORS\n",
        "  accessor = BATCH_ACCESSORS[name]()\n",
        "  return accessor\n",
        "\n",
        "def register_batch_accessor(name): \n",
        "  def register(cls):\n",
        "    print(f\"Registering Batch Accessor {name}: {cls}\")\n",
        "    global BATCH_ACCESSORS\n",
        "    BATCH_ACCESSORS[name] = cls\n",
        "    return cls\n",
        "  return register\n",
        "\n",
        "@register_batch_accessor(DEFAULT_BATCH_ACCESSOR)\n",
        "class PyDictNativeDataAccessor:\n",
        "  def fetch_batch(self, data: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "    return RowBatch(data, len(data))\n",
        "\n",
        "@register_batch_accessor(\"pydict_arrow_batch_accessor\")\n",
        "class PyDictArrowDataAccessor:\n",
        "  def fetch_batch(self, data: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "    return PyDictArrowDataAccessor.build_columns(input=data, block_size=block_size, start=start)\n",
        "\n",
        "  @staticmethod\n",
        "  def build_column_val(input: List[Any]) -> pyarrow.Array:\n",
        "      child_type = struct_type(next(iter(input)))\n",
        "      assert child_type != StructType.NON_LEAF, f\"Invalid input {input}\"\n",
        "      # List[List[int]]\n",
        "      if child_type == StructType.LEAF_ID_LIST:\n",
        "          col = pyarrow.ListArray.from_arrays(\n",
        "              offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "              values=pyarrow.array(list(itertools.chain(*input)), type=pyarrow.int64()),\n",
        "          )\n",
        "      # List[float]\n",
        "      elif child_type == StructType.LEAF_FLOAT:\n",
        "          col = pyarrow.array(input, type=pyarrow.float32())\n",
        "      # List[Dict[int, float]]\n",
        "      elif child_type == StructType.LEAF_ID_SCORE_MAP:\n",
        "          col = pyarrow.MapArray.from_arrays(\n",
        "              offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "              keys=pyarrow.array(list(itertools.chain(*[item.keys() for item in input])), type=pyarrow.int64()),\n",
        "              items=pyarrow.array(list(itertools.chain(*[item.values() for item in input])), type=pyarrow.float32()),\n",
        "          )\n",
        "      return col\n",
        "\n",
        "  @staticmethod\n",
        "  def build_columns(input: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "      assert len(input) > 0, f\"input {input} cannot have empty columns\"\n",
        "      col_values = []\n",
        "      row_num = block_size\n",
        "      for val in input.values():\n",
        "          if type(val) == list:\n",
        "              block_size = block_size or len(val)\n",
        "              end = min(start + block_size, len(val))\n",
        "              row_num = end - start\n",
        "              col_val = PyDictArrowDataAccessor.build_column_val(val[start:end])\n",
        "          elif type(val) == dict:\n",
        "              col_val = PyDictArrowDataAccessor.build_columns(val, block_size, start)\n",
        "              row_num = col_val.row_num\n",
        "          col_values.append(col_val)\n",
        "\n",
        "      return RowBatch(data=zip(input.keys(), col_values), row_num=row_num)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIwifsz6I2uf",
        "outputId": "2f291fec-18af-45f4-d449-d982646a3a8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registering Batch Accessor native_batch_accessor: <class '__main__.PyDictNativeDataAccessor'>\n",
            "Registering Batch Accessor pydict_arrow_batch_accessor: <class '__main__.PyDictArrowDataAccessor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RowBatchIterator(Iterator[RowBatch]):\n",
        "  def __init__(self, fetch_fn: Callable[[Dict[str, Any]], RowBatch], states: Dict[str, Any]) -> None:\n",
        "    self._fetch_fn = fetch_fn\n",
        "    self._orig_states = states\n",
        "    self._states = states\n",
        "\n",
        "  def __iter__(self) -> \"RowBatchIterator\": \n",
        "    # reset iterator\n",
        "    self._states = self._orig_states\n",
        "    return self\n",
        "\n",
        "  def __next__(self) -> RowBatch:\n",
        "    data, self._states = self._fetch_fn(self._states)\n",
        "    return data\n",
        "\n",
        "\n",
        "class ShuffleSpec(Enum):\n",
        "  NO_SHUFFLE = 0\n",
        "  FULL_SHUFFLE = 1\n",
        "\n",
        "\n",
        "class Datapipe(ABC):\n",
        "  def __init__(self) -> None:\n",
        "    self._batch_size = 3\n",
        "    self._batch_accessor = DEFAULT_BATCH_ACCESSOR\n",
        "\n",
        "  # Commenting out all plan building logic for now.\n",
        "  def shuffle(self, shuffle_spec: ShuffleSpec) -> \"Datapipe\":\n",
        "    self._shuffle_spec = shuffle_spec\n",
        "    # self._plan = PlanBuilder.addShuffle(shuffle_spec)\n",
        "    return self\n",
        "\n",
        "  def repeat(self, num: int) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addRepeat(num)\n",
        "    self._total_passes = num\n",
        "    return self\n",
        "\n",
        "  def sort(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addSort(fn)\n",
        "    return self\n",
        "\n",
        "  def map_batches(\n",
        "      self, \n",
        "      fn: Callable[[RowBatch], RowBatch], \n",
        "      batch_size: Optional[int] = None,\n",
        "      batch_accessor: Optional[str] = None,\n",
        "    ) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addMapBatches(fn, batch_size)\n",
        "    self._map_batch_fn = fn\n",
        "    self._batch_size = batch_size or self._batch_size\n",
        "    self._batch_accessor = batch_accessor or self._batch_accessor\n",
        "    return self\n",
        "\n",
        "  # can be implemented with map_batches()\n",
        "  def filter(self, fn: Callable[[Row], bool]) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addFilter(fn)\n",
        "    return self\n",
        "\n",
        "  # can be implemented with map_batches()\n",
        "  def map(self, fn: Callable[[Row], Row]) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.addMap(fn)\n",
        "    return self\n",
        "\n",
        "  def rebatch(self, batch_size: int) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.rebatch(batch_size)\n",
        "    self._batch_size = batch_size\n",
        "    return self\n",
        "\n",
        "  def change_batch_accessor(self, batch_accessor: str) -> \"Datapipe\":\n",
        "    # self._plan = PlanBuilder.batchFormat(batch_format)\n",
        "    self._batch_accessor = batch_accessor\n",
        "    return self\n",
        "\n",
        "  # aggregation functions like max, mean, min, sum, etc. will be added later\n",
        "\n",
        "  @property\n",
        "  def batch_size(self) -> int:\n",
        "    return self._batch_size\n",
        "\n",
        "# IterDatapipe -- iterate over data source to produce data. \n",
        "# Used in training together with Dataloader and ReadingService\n",
        "class IterDatapipe(Datapipe):\n",
        "  @abstractmethod\n",
        "  def iter_batches(self, batch_size: int, start: int = 0, stride: int = 1) -> RowBatchIterator:\n",
        "    pass\n",
        "\n",
        "\n",
        "class PyDictDatapipe(IterDatapipe): \n",
        "  def __init__(self, data: Dict[str, Any], batch_size: int, batch_accessor: str) -> None:\n",
        "    super().__init__()\n",
        "    self._data = data\n",
        "    self._batch_size = batch_size\n",
        "    self._batch_accessor = batch_accessor\n",
        "\n",
        "  def fetch_batch(self, states: Dict[str, Any]) -> Tuple[RowBatch, Dict[str, Any]]:\n",
        "    # The current implementation is for demo purpose. \n",
        "    # Real implementation should be based on executing self._plan rather than calling specific functions\n",
        "    batch = get_batch_accessor(self._batch_accessor).fetch_batch(self._data, states[\"batch_size\"], states[\"start\"])\n",
        "    if hasattr(self, \"_map_batch_fn\"):\n",
        "      batch = self._map_batch_fn(batch)\n",
        "    states[\"start\"] += states[\"batch_size\"] * states[\"stride\"]\n",
        "    return (batch, states)\n",
        "\n",
        "  def iter_batches(self, batch_size: int, start: int = 0, stride: int = 1) -> RowBatchIterator:\n",
        "    return RowBatchIterator(self.fetch_batch, {\"start\": start, \"batch_size\": batch_size, \"stride\": stride})\n",
        "\n",
        "# class HiveDatapipe(IterDatapipe):\n",
        "#   def __init__(self, hive_specs: Dict[str, Any], block_size: int, compute_adaptor) -> None: \n",
        "\n",
        "# CallableDatapipe -- Apply transformation on data. \n",
        "# Action in reactive way (need to be called with provided data). Userd in inference\n",
        "class CallableDatapipe(Datapipe):\n",
        "  def __call__(self, data):\n",
        "    raise NotImplementedError(\"not supporting customized schema\")\n",
        "\n",
        "class SchemaDatapipe(CallableDatapipe):\n",
        "  def __init__(self, schema: Dict[str, Any], batch_accessor: str) -> None:\n",
        "    super().__init__()\n",
        "    self._schema = schema\n",
        "    self._batch_accessor = batch_accessor\n",
        "\n",
        "  def conversion(self, data) -> RowBatch:\n",
        "    # The current implementation is for demo purpose. \n",
        "    # Real implementation should be based on executing self._plan rather than calling specific functions\n",
        "    batch = get_batch_accessor(self._batch_accessor).fetch_batch(data, None, 0)\n",
        "    if hasattr(self, \"_map_batch_fn\"):\n",
        "      batch = self._map_batch_fn(batch)\n",
        "    return batch\n",
        "\n",
        "  def __call__(self, data) -> RowBatch: \n",
        "    return self.conversion(data)"
      ],
      "metadata": {
        "id": "VuQFXjQWrKKG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Data"
      ],
      "metadata": {
        "id": "JpvPivsgZuwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SCHEMA = (\n",
        "    (\"float_features\", Dict[str, List[float]]),\n",
        "    (\"float_features\", Dict[str, List[List[int]]]),\n",
        "    (\"id_score_list_features\", Dict[str, List[Dict[int, float]]]),\n",
        ")\n",
        "\n",
        "\n",
        "# dp = torch.data.from_hive(table_ns=\"ai_platform\", table_nm=\"table_1\", partitions=[\"ds='2022-04-23'\"])\n",
        "# 2 columns, 6 rows with 3 rows being a block\n",
        "dp = PyDictDatapipe(\n",
        "    data={\n",
        "        \"float_features\": {\n",
        "            \"f1\": [1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
        "            \"f2\": [2.0, 2.1, 2.2, 2.3, 2.4, 2.5],\n",
        "        },\n",
        "        \"id_list_features\": {\n",
        "            \"id1\": [[111, 112], [121], [131, 132], [], [151], [161]],\n",
        "            \"id2\": [[], [221], [222], [223, 224], [225], []],\n",
        "            \"id3\": [[311], [], [331], [341], [351], [361]],\n",
        "        },\n",
        "        \"id_score_list_features\": {\n",
        "            \"ids1\": [{411: 0.1}, {421: 0.2}, {431: 0.7}, {}, {451: 0.4, 452: 0.1}, {}],\n",
        "        },\n",
        "    },\n",
        "    batch_size=4,\n",
        "    batch_accessor=\"pydict_arrow_batch_accessor\",\n",
        ")"
      ],
      "metadata": {
        "id": "APQLSL-gs3ik"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_ACCESSORS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAOs16GwbGFQ",
        "outputId": "b5440563-c6a6-400f-cda8-331badc1debb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'native_batch_accessor': __main__.PyDictNativeDataAccessor,\n",
              " 'pydict_arrow_batch_accessor': __main__.PyDictArrowDataAccessor}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test single iter\n",
        "rb = next(iter(dp.iter_batches(batch_size=4, start=0, stride=1)))\n",
        "type(rb)\n",
        "rb.row_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihOdG6wptOPC",
        "outputId": "241e9523-1659-45a4-9b21-29014bf8bcd8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.compute as F\n",
        "\n",
        "print(rb[\"float_features\"][\"f1\"])\n",
        "print(F.ln(F.add(rb[\"float_features\"][\"f1\"], 1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etchmkzns8bR",
        "outputId": "d75b2bf7-8aa4-42a8-ddd4-a9c8ad059a87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  1,\n",
            "  1.1,\n",
            "  1.2,\n",
            "  1.3\n",
            "]\n",
            "[\n",
            "  0.6931472,\n",
            "  0.7419373,\n",
            "  0.7884574,\n",
            "  0.8329091\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test iter_batches\n",
        "for batch in dp.iter_batches(batch_size=5):\n",
        "  print(batch[\"float_features\"][\"f1\"])\n",
        "  print(batch.row_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PQykZIjzChC",
        "outputId": "c7331fe9-4d7d-4cd0-c99d-9674e43f7748"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  1,\n",
            "  1.1,\n",
            "  1.2,\n",
            "  1.3,\n",
            "  1.4\n",
            "]\n",
            "5\n",
            "[\n",
            "  1.5\n",
            "]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReadingService\n",
        "Specify the data reading execution engine. It should be STATELESS\n",
        "\n",
        "\n",
        "* SimpleReadingService\n",
        "* DisaggDppReadingService\n",
        "* OnboxDppReadingService\n",
        "* OnboxMultiprocessReadingService"
      ],
      "metadata": {
        "id": "w1n7y89owsf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReadingService(ABC):\n",
        "  def __init__(self) -> None:\n",
        "    \n",
        "    return\n",
        "\n",
        "  def initialize(self, dp: IterDatapipe) -> IterDatapipe:\n",
        "    raise NotImplementedError(\"Not implement initialize()\")\n",
        "\n",
        "  def finalize(self) -> bool:\n",
        "    return True\n",
        "\n",
        "  def iter_batches(self, dp: IterDatapipe, state: Dict[str, Any]) -> RowBatchIterator:\n",
        "    raise NotImplementedError(\"Not implement read_batch()\")\n",
        "\n",
        "\n",
        "class SimpleReadingService(ReadingService):\n",
        "  def initialize(self, dp: Datapipe) -> IterDatapipe:\n",
        "    return dp\n",
        "\n",
        "  def iter_batches(self, dp: IterDatapipe, state: Dict[str, Any]) -> RowBatchIterator:\n",
        "    yield from dp.iter_batches(batch_size=dp.batch_size, start=state[\"start\"], stride=1)\n"
      ],
      "metadata": {
        "id": "zWEy4RyCwqWW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader\n",
        "* Take Datapipe (describing business logic) and ReadingService (specifying execution engine) to produce data batches. \n",
        "* Dataloader is STATEFUL\n",
        "* Provide checkpointing interface"
      ],
      "metadata": {
        "id": "EMjCqevIx_GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataloader:\n",
        "  def __init__(self, datapipe: Datapipe, reading_service: ReadingService) -> None:\n",
        "    self._dp = datapipe\n",
        "    self._rs = reading_service\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self) -> None:\n",
        "    self._states = {\n",
        "        \"start\": 0,\n",
        "    }\n",
        "    self._dp = self._rs.initialize(self._dp)\n",
        "\n",
        "  def iter_batches(self) -> RowBatchIterator:\n",
        "    # TODO: need to update self._states appropriately\n",
        "    yield from self._rs.iter_batches(self._dp, self._states)\n",
        "\n",
        "  def __iter__(self) -> \"Dataloader\":\n",
        "    self.reset()\n",
        "    return self\n",
        "\n",
        "  def __next__(self) -> RowBatch:\n",
        "    return next(self.iter_batches())\n",
        "\n",
        "  def state_dict(self) -> Dict[str, Any]:\n",
        "    return self._states\n",
        "\n",
        "  def load_state_dict(self, states: Dict[str, Any]) -> bool:\n",
        "    self._states = states\n",
        "    return True"
      ],
      "metadata": {
        "id": "GmJeviKSyQqK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "jhzqtpmrbEc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow.compute as F\n",
        "\n",
        "def collate(rows: RowBatch) -> RowBatch:\n",
        "  return rows.to_torch()\n",
        "\n",
        "def preproc(rows: RowBatch) -> RowBatch:\n",
        "    rows[\"float_features\"][\"f1\"] = F.ln(F.add(rows[\"float_features\"][\"f1\"], 3))\n",
        "    # can collate in preproc. Or can collate in training loop.\n",
        "    output = collate(rows)\n",
        "    return output\n",
        "\n",
        "data={\n",
        "    \"float_features\": {\n",
        "        \"f1\": [1.0, 1.1, 1.2, 1.3, 1.4, 1.5],\n",
        "        \"f2\": [2.0, 2.1, 2.2, 2.3, 2.4, 2.5],\n",
        "    },\n",
        "    \"id_list_features\": {\n",
        "        \"id1\": [[111, 112], [121], [131, 132], [], [151], [161]],\n",
        "        \"id2\": [[], [221], [222], [223, 224], [225], []],\n",
        "        \"id3\": [[311], [], [331], [341], [351], [361]],\n",
        "    },\n",
        "    \"id_score_list_features\": {\n",
        "        \"ids1\": [{411: 0.1}, {421: 0.2}, {431: 0.7}, {}, {451: 0.4, 452: 0.1}, {}],\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "lW43mh7GJCsc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp = PyDictDatapipe(\n",
        "    data=data,\n",
        "    batch_size=4,\n",
        "    batch_accessor=\"pydict_arrow_batch_accessor\",\n",
        ")\n",
        "# dp = dp.repeat(num=2)\n",
        "# dp = dp.shuffle(shuffle_mode=ShuffleMode.SHAFFLE_ALL)\n",
        "dp = dp.map_batches(preproc, batch_size=5)\n",
        "\n",
        "rs = SimpleReadingService()\n",
        "\n",
        "dl = Dataloader(dp, rs)\n",
        "\n",
        "for idx, batch in enumerate(dl.iter_batches()):\n",
        "  # print(f\"==Batch {idx}, size {batch.row_num}==\")\n",
        "  # input = batch.to_torch()\n",
        "  print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LRn17KihduG",
        "outputId": "1e5321f5-2e8d-42d0-a97f-fba14d80219f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'float_features': {'f1': (tensor([1, 1, 1, 1, 1], dtype=torch.int8), tensor([1.3863, 1.4110, 1.4351, 1.4586, 1.4816])), 'f2': (tensor([1, 1, 1, 1, 1], dtype=torch.int8), tensor([2.0000, 2.1000, 2.2000, 2.3000, 2.4000]))}, 'id_list_features': {'id1': (tensor([0, 2, 3, 5, 5, 6], dtype=torch.int32), tensor([111, 112, 121, 131, 132, 151])), 'id2': (tensor([0, 0, 1, 2, 4, 5], dtype=torch.int32), tensor([221, 222, 223, 224, 225])), 'id3': (tensor([0, 1, 1, 2, 3, 4], dtype=torch.int32), tensor([311, 331, 341, 351]))}, 'id_score_list_features': {'ids1': (tensor([0, 1, 2, 3, 3, 5], dtype=torch.int32), tensor([411, 421, 431, 451, 452]), tensor([0.1000, 0.2000, 0.7000, 0.4000, 0.1000]))}}\n",
            "{'float_features': {'f1': (tensor([1], dtype=torch.int8), tensor([1.5041])), 'f2': (tensor([1], dtype=torch.int8), tensor([2.5000]))}, 'id_list_features': {'id1': (tensor([0, 1], dtype=torch.int32), tensor([161])), 'id2': (tensor([0, 0], dtype=torch.int32), tensor([], dtype=torch.int64)), 'id3': (tensor([0, 1], dtype=torch.int32), tensor([361]))}, 'id_score_list_features': {'ids1': (tensor([0, 0], dtype=torch.int32), tensor([], dtype=torch.int64), tensor([]))}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MaM0SUoVjrws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infernece"
      ],
      "metadata": {
        "id": "nr66zYTmibkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dp = SchemaDatapipe(schema=INPUT_SCHEMA, batch_accessor=\"pydict_arrow_batch_accessor\")\n",
        "dp = dp.map_batches(preproc)\n",
        "pred = dp(data)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfUnQpBsidOO",
        "outputId": "baf58be5-3dd5-4ab3-a693-5e2e6b7b3169"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'float_features': {'f1': (tensor([1, 1, 1, 1, 1, 1], dtype=torch.int8),\n",
              "   tensor([1.3863, 1.4110, 1.4351, 1.4586, 1.4816, 1.5041])),\n",
              "  'f2': (tensor([1, 1, 1, 1, 1, 1], dtype=torch.int8),\n",
              "   tensor([2.0000, 2.1000, 2.2000, 2.3000, 2.4000, 2.5000]))},\n",
              " 'id_list_features': {'id1': (tensor([0, 2, 3, 5, 5, 6, 7], dtype=torch.int32),\n",
              "   tensor([111, 112, 121, 131, 132, 151, 161])),\n",
              "  'id2': (tensor([0, 0, 1, 2, 4, 5, 5], dtype=torch.int32),\n",
              "   tensor([221, 222, 223, 224, 225])),\n",
              "  'id3': (tensor([0, 1, 1, 2, 3, 4, 5], dtype=torch.int32),\n",
              "   tensor([311, 331, 341, 351, 361]))},\n",
              " 'id_score_list_features': {'ids1': (tensor([0, 1, 2, 3, 3, 5, 5], dtype=torch.int32),\n",
              "   tensor([411, 421, 431, 451, 452]),\n",
              "   tensor([0.1000, 0.2000, 0.7000, 0.4000, 0.1000]))}}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archived"
      ],
      "metadata": {
        "id": "vwC-VypRDYZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class Table(ABC, dict):\n",
        "    def __init__(self, data: Dict[str, Any], block_size: int) -> None: # , start: int = 0, stride: int = 1) -> None:\n",
        "        self._data = data\n",
        "        self._block_size = block_size\n",
        "        # self._start = start\n",
        "        # self._stride = stride\n",
        "\n",
        "    # def __iter__(self) -> RowBatchIterator:\n",
        "    #     return RowBatchIterator(self, self._start, self._block_size, self._stride)\n",
        "\n",
        "    # TODO: wonder if we should have iterator method in Table?\n",
        "    #       considering Table is an \"accessors of data\", but not necessary storing the real data, having it seems reasonable\n",
        "    def iter_batches(self, batch_size: int, start: int = 0, stride: int = 1) -> RowBatchIterator:\n",
        "      return RowBatchIterator(self, start=start, batch_size=batch_size, stride=stride)\n",
        "\n",
        "    # TODO: this fucntion will be called by RowBatchIterator.__next__(). Seems a bit cyclic...\n",
        "    @abstractmethod\n",
        "    def fetch_batch(self, start: int, batch_size: int) -> RowBatch:\n",
        "      ...\n",
        "\n",
        "\n",
        "class ArrowTable(Table):\n",
        "    # def __next__(self) -> RowBatch:\n",
        "    #     columns = ArrowTable.build_columns(self._data, self._block_size, self._start)\n",
        "    #     self._start += self._block_size * self._stride\n",
        "    #     return columns\n",
        "    \n",
        "    def fetch_batch(self, start: int, batch_size: int) -> RowBatch:\n",
        "      return ArrowTable.build_columns(self._data, batch_size, start)\n",
        "\n",
        "    @staticmethod\n",
        "    def build_column_val(input: List[Any]) -> pyarrow.Array:\n",
        "        child_type = struct_type(next(iter(input)))\n",
        "        assert child_type != StructType.NON_LEAF, f\"Invalid input {input}\"\n",
        "        # List[List[int]]\n",
        "        if child_type == StructType.LEAF_ID_LIST:\n",
        "            col = pyarrow.ListArray.from_arrays(\n",
        "                offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "                values=list(itertools.chain(*input)),\n",
        "            )\n",
        "        # List[float]\n",
        "        elif child_type == StructType.LEAF_FLOAT:\n",
        "            col = pyarrow.array(input, type=pyarrow.float32())\n",
        "        # List[Dict[int, float]]\n",
        "        elif child_type == StructType.LEAF_ID_SCORE_MAP:\n",
        "            col = pyarrow.MapArray.from_arrays(\n",
        "                offsets=pyarrow.array(compute_offsets(input, True)),\n",
        "                keys=list(itertools.chain(*[item.keys() for item in input])),\n",
        "                items=list(itertools.chain(*[item.values() for item in input])),\n",
        "            )\n",
        "        return col\n",
        "\n",
        "    @staticmethod\n",
        "    def build_columns(input: Dict[str, Collection], block_size: Optional[int] = None, start: int = 0) -> RowBatch:\n",
        "        assert len(input) > 0, f\"input {input} cannot have empty columns\"\n",
        "        col_values = []\n",
        "        row_num = block_size\n",
        "        for val in input.values():\n",
        "            if type(val) == list:\n",
        "                block_size = block_size or len(val)\n",
        "                end = min(start + block_size, len(val))\n",
        "                row_num = end - start\n",
        "                col_val = ArrowTable.build_column_val(val[start:end])\n",
        "            elif type(val) == dict:\n",
        "                col_val = ArrowTable.build_columns(val, block_size, start)\n",
        "                row_num = col_val.row_num\n",
        "            col_values.append(col_val)\n",
        "\n",
        "        return RowBatch(data=zip(input.keys(), col_values), row_num=row_num)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_pydict(input: Dict[str, Any], block_size: int) -> \"ArrowTable\":\n",
        "        return ArrowTable(data=input, block_size=block_size)"
      ],
      "metadata": {
        "id": "PcvYtV_CDXf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}